{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Contents :</p> <ol> <li>Introduction to Numerical Linear Algebra</li> <li>Direct Methods</li> <li>Condition and Stability</li> <li>Eigenvalues Solving</li> <li>Iterative Methods</li> <li>Miscellaneous Topics</li> </ol>"},{"location":"def/Eigenvalue_and_Eigenvector/","title":"Eigen Value and Vecotr","text":"<p>Definition : Consider a square matrix \\(A=[a_{ij}]_{n{\\times}n}\\) and a nonzero vector \\(v\\) of length \\(n\\) Then there exit a scalar \\(\\lambda \\in R\\) such that \\(Av = \\lambda v\\)  . Where \\(v\\) is called eigenvector corresponding to eigenvalue \\(\\lambda\\) of matrix \\(A\\).</p> <p>Remark :  1. Linear transformation \\(T : R^n \\to R^n\\) is equivalent to the square matrix \\(A\\) of order \\(n \\times n\\). thus given a basis set of the vector space can be defined as set of eigen vectors of matrix \\(A\\) for linear transformation \\(T\\).</p> <ol> <li>Eigenvectors and eigenvalues exits in a wide range of applications  like stability and vibration analysis of dynamical systems, atomic orbits, facial recognition and matrix diagonalization.</li> </ol>"},{"location":"def/Eigenvalue_and_Eigenvector/#faddeev-leverrier-algorithm","title":"Faddeev-LeVerrier Algorithm","text":"<p>Faddeev-LeVerrier algorithm is a recursive method to calculate the coefficients of the characteristic polynomial  \\(p_A(\\lambda)=\\det (\\lambda I_n - A)\\)  of a square matrix, A. solving characteristic polynomial gives eigen values of matrix A as a roots of it and matrix polynomial of matrix A  vanishes i.e p(A) = 0 by Cayley-Hamilton Theorem. Faddeev-Le Verrier algorithm works directly with coefficients of matrix \\(A\\).</p> <p>Problem : Given a </p>"},{"location":"def/Linear_Transformations/","title":"Linear Transformations","text":"<p>Given two linear space V and W over a field F, a linear map \\(T:V\\to W\\) that preserve addition and scalar multiplication such that</p> \\[ \\begin{equation} T(\\mathbf u + \\mathbf v)=T(\\mathbf u)+T(\\mathbf v)  \\\\  \\quad T(a \\mathbf v)=aT(\\mathbf v) \\end{equation} \\] <p>or we can write in general via linear combination as</p> \\[ \\\\ \\; \\begin{equation} T(a \\mathbf u + b \\mathbf v)= T(a \\mathbf u) + T(b \\mathbf v) = aT(\\mathbf u) + bT(\\mathbf v)  \\end{equation} \\] <p>for all \\(u, v \\in V\\) and scaler \\(a, b \\in F\\)</p> <p>remark-1 : If \\(V = W\\) are the same linear space then linear map \\(T\u00a0: V \u2192 V\\) is also known as a linear operator on \\(V\\).</p> <p>remark-2 : A bijective linear map between two linear spaces  is an isomorphism because it preserves linear structure and two isomorphic linear spaces are same algebraically means we can't make any distiction between these two spaces using linear space properties.</p> <p>remark-3 : How to check wheather a linear map is isomorphic or not. If it is non-isomorphic then we find its range space(set of elements which have non-zero images) and null space(set of elements which have zero images also called kernel of T).</p>"},{"location":"def/MatrixComputation/","title":"Matrix Computation","text":"<p>Understanding Errors in Numerical Computation</p> <p>Errors in numerical computation arise from various sources, such as modeling inaccuracies, measurement noise, manufacturing imperfections, and limitations in computational precision. These errors are compounded when data must be represented in finite precision formats like floating-point arithmetic, which introduces rounding errors. Additional errors may emerge from truncating infinite mathematical processes into finite iterative computations. These challenges are particularly relevant in matrix computations, where the sensitivity of algorithms to errors depends on factors like vector and matrix norms, floating-point representation standards, condition numbers, and numerical stability. Norms quantify the size of vectors and matrices, providing a way to express error bounds, while condition numbers measure how much small changes in input can affect computational results. Numerical stability ensures that rounding errors introduced during computation do not grow excessively, safeguarding the reliability of results even under finite precision constraints.</p> <p>Ensuring Reliable Computations Through Standards and Methods Floating-point arithmetic, as standardized by IEEE (e.g., IEEE 754 and IEEE 854), defines how real numbers are approximated and manipulated in computer systems. These standards provide a framework for understanding how rounding errors arise and propagate during computations. Tools like norms, condition numbers, and perturbation analysis are used to analyze and mitigate these errors. For example, a matrix's condition number determines whether the problem it represents is well-posed or ill-posed, influencing the choice of algorithm. Stable algorithms, which minimize the amplification of rounding errors, are critical in ensuring accurate results even for large or sensitive problems. Iterative methods like the power method, inverse iteration, and orthogonal iteration play significant roles in computing eigenvalues and eigenvectors, balancing accuracy, stability, and computational efficiency. When coupled with robust theoretical frameworks, such as those outlined in seminal works by Wilkinson, Higham, and others, these tools enable effective handling of errors in numerical computations, ensuring precision in both real and complex vector spaces.</p>"},{"location":"def/MatrixComputation/#conditioning-and-condition-numbers","title":"Conditioning and Condition Numbers","text":"<p>Errors are inevitable in real-world data due to measurement imprecision, equipment degradation, manufacturing tolerances, and limitations of floating-point arithmetic. These errors propagate through calculations, influencing the precision of computational results. The field of conditioning studies how errors in input data affect the results of computations.</p>"},{"location":"def/MatrixComputation/#conditioning-and-condition-numbers_1","title":"Conditioning and Condition Numbers","text":"<p>Errors are inevitable in real-world data due to measurement imprecision, equipment degradation, manufacturing tolerances, and limitations of floating-point arithmetic. These errors propagate through calculations, influencing the precision of computational results. The field of conditioning studies how errors in input data affect the results of computations. Below is a summary of the key concepts:</p> <ol> <li> <p>Computational Problem:    A computational problem involves evaluating a function \\(P: \\mathbb{R}^n \\to \\mathbb{R}^m\\) at a given data point \\(z \\in \\mathbb{R}^n\\). In practice, \\(z\\) is often approximated due to errors, represented as \\(\\hat{z} \\in \\mathbb{R}^n\\).</p> </li> <li> <p>Error Measures: </p> </li> <li>Absolute Error:      $      \\text{Absolute error} = |z - \\hat{z}|.      $</li> <li> <p>Relative Error:      $      \\text{Relative error} = \\frac{|z - \\hat{z}|}{|z|}, \\quad z \\neq 0.      $      If \\(z = 0\\), the relative error is undefined.</p> </li> <li> <p>Conditioning of Data: </p> </li> <li>Well-Conditioned Data:      Small relative perturbations in $ z $ lead to small relative perturbations in $ P(z) $.</li> <li>Ill-Conditioned Data:      Even small relative perturbations in $ z $ may cause large relative perturbations in $ P(z) $.    The distinction depends on the context and precision required for the task.</li> </ol>"},{"location":"def/MatrixComputation/#condition-number","title":"Condition Number:","text":"<p>To quantify the sensitivity of a problem to input perturbations, the condition number is defined.</p> <ol> <li> <p>Relative Condition Number:    For $z \\in \\mathbb{R}^n $ and $ P(z) \\neq 0 $, the relative condition number is given by:    $$    \\text{cond}P(z) = \\lim{\\epsilon \\to 0} \\sup \\frac{|P(z + \\delta z) - P(z)| / |P(z)|}{|\\delta z| / |z|}, \\quad |\\delta z| \\leq \\epsilon.    $$    This measures the largest possible relative change in $ P(z) $ resulting from small relative changes in $ z $.</p> </li> <li> <p>Extension for $ z = 0 $ or Isolated Roots:    When $ z = 0 $ or $ z $ is an isolated root of $ P(z) $, the condition number can be generalized as:    $$    \\text{cond}P(z) = \\limsup{x \\to z} \\text{cond}_P(x).    $$</p> </li> <li> <p>Key Notes: </p> </li> <li>The condition number depends on the function $ P $, the input data $ z $, and the norm used in the computation.</li> <li>It characterizes the sensitivity of $ z $, not $ P(z) $, and not the algorithm used to compute $ P(z) $.</li> </ol>"},{"location":"def/MatrixComputation/#importance-of-conditioning","title":"Importance of Conditioning:","text":"<ul> <li>Well-conditioned data ensures reliable computational results with limited precision.</li> <li>Ill-conditioned data may lead to significant errors or instability in computations, requiring additional measures such as robust algorithms or improved precision in inputs.</li> </ul>"},{"location":"def/MatrixComputation/#key-facts-about-conditioning-and-condition-numbers","title":"Key Facts About Conditioning and Condition Numbers","text":""},{"location":"def/MatrixComputation/#1-impact-of-finite-precision-and-rounding-errors","title":"1. Impact of Finite Precision and Rounding Errors:","text":"<ul> <li>Ubiquity of Rounding Errors:   In any finite precision computation, the best achievable result is $ P(z + \\delta z) $, where $ |\\delta z| \\leq \\epsilon |z| $, and $ \\epsilon $ is a small multiple of the machine's floating-point unit round-off.   (Refer to Section 37.6 for more details.)</li> </ul>"},{"location":"def/MatrixComputation/#2-relative-condition-number-and-error-bounds","title":"2. Relative Condition Number and Error Bounds:","text":"<ul> <li>Asymptotic Relative Error Bound:   The relative condition number provides an asymptotic bound for the relative error:   $$   \\frac{|P(z + \\delta z) - P(z)|}{|P(z)|} \\leq \\text{cond}_P(z) \\frac{|\\delta z|}{|z|} + o\\left(\\frac{|\\delta z|}{|z|}\\right),   $$   as $ |\\delta z| \\to 0 $.  </li> <li>Significant Digits and Condition Number:   If the condition number satisfies $ \\text{cond}_P(z) \\approx 10^s $, then roughly $ s $ significant digits are lost in the computed result $ P(z) $. For example, if the input data $ z $ has $ p $ correct digits, the result $ P(z) $ retains approximately $ p - s $ correct digits.</li> </ul>"},{"location":"def/MatrixComputation/#3-condition-number-via-frechet-derivatives","title":"3. Condition Number via Fr\u00e9chet Derivatives:","text":"<ul> <li>Definition via Derivatives:   For $ P $ having a Fr\u00e9chet derivative $ D(z) $ at $ z \\in \\mathbb{F}^n $, the relative condition number is:   $$   \\text{cond}_P(z) = \\frac{|D(z)| \\cdot |z|}{|P(z)|}.   $$</li> <li>Condition Number for Scalar Functions:   If $ f(x) $ is a smooth real function of a real variable $ x $, the condition number simplifies to:   $$   \\text{cond}_f(z) = \\left| \\frac{z f'(z)}{f(z)} \\right|.   $$   This measures the sensitivity of $ f(z) $ to small perturbations in $ z $.</li> </ul> <p>### Example of Conditioning for $ P(x) = \\sin(x) $</p> <ol> <li>Error Amplification in $ \\sin(z) $:    Given $ z = \\frac{22}{7} $, the data point has an uncertainty of approximately $ \\pi - \\frac{22}{7} \\approx 0.00126 $.    Since $ \\sin(x) $ is a periodic function with high sensitivity near certain values (e.g., multiples of $ \\pi $), the relative error in $ \\sin(z) $ can be significant.    Specifically, for $ z = \\frac{22}{7} $, the relative error in $ \\sin(z) $ can reach 100%, making $ z $ highly ill-conditioned with respect to $ \\sin(z) $. </li> </ol> <ol> <li>Condition Number for $ \\sin(x) $:    The condition number of $ z $ with respect to $ \\sin(z) $ is given by:    $$    \\text{cond}{\\sin}(z) = |z \\cot(z)|.    $$    For $ z = \\frac{22}{7} $, we calculate:    $$    \\text{cond}{\\sin}(22/7) \\approx 2485.47.    $$</li> </ol> <ol> <li>Relative Error Bound for Perturbation:    If $ z $ is perturbed to $ z + \\delta z = \\pi $, the asymptotic relative error bound from Fact 2 is:    $$    \\frac{\\sin(z + \\delta z) - \\sin(z)}{\\sin(z)} \\leq \\text{cond}_{\\sin}(z) \\frac{\\delta z}{z} + o\\left(\\frac{\\delta z}{z}\\right).    $$    Substituting the values:    $$    \\frac{\\sin(z + \\delta z) - \\sin(z)}{\\sin(z)} = 1.    $$    This shows the actual relative error reaches its theoretical upper bound, confirming the ill-conditioned nature of $ z = \\frac{22}{7} $ with respect to $ \\sin(z) $.</li> </ol> <p>### Conditioning Examples in Numerical Computation</p>"},{"location":"def/MatrixComputation/#2-subtractive-cancellation","title":"2. Subtractive Cancellation:","text":"<p>For $ x \\in \\mathbb{R}^2 $, define the computational problem: $$ P(x) = [1, -1]x = x_1 - x_2. $$</p> <ul> <li> <p>Gradient of $ P(x) $:   The gradient is constant and independent of $ x $:   $$   \\nabla P(x) = [1, -1].   $$</p> </li> <li> <p>Condition Number:   Using the $ \\infty $-norm and applying Fact 3, the condition number is:   $$   \\text{cond}{P}(x) = \\frac{|\\nabla P(x)|\\infty |x|\\infty}{|P(x)|}.   $$   Substituting the expressions:   $$   \\text{cond}{P}(x) = \\frac{2 \\max(|x_1|, |x_2|)}{|x_1 - x_2|}.   $$</p> </li> <li> <p>Analysis:   This condition number becomes large when $ x_1 \\approx x_2 $, indicating ill-conditioning.   This reflects the challenge of subtractive cancellation, where small differences between nearly equal values $ x_1 $ and $ x_2 $ can lead to significant relative errors in $ P(x) $.</p> </li> </ul>"},{"location":"def/MatrixComputation/#3-conditioning-of-matrixvector-multiplication","title":"3. Conditioning of Matrix\u2013Vector Multiplication:","text":"<p>For a fixed matrix $ A \\in \\mathbb{F}^{m \\times n} $, define the computational problem: $$ P(x) = Ax, \\quad \\text{where } x \\in \\mathbb{F}^n. $$</p> <ul> <li> <p>Relative Condition Number:   The condition number of $ x $ with respect to $ P(x) $ is:   $$   \\text{cond}(x) = \\frac{|A| |x|}{|Ax|},   $$   where the matrix norm $ |A| $ is the operator norm induced by the chosen vector norm $ |\\cdot| $.</p> </li> <li> <p>Special Case (Square and Nonsingular $ A $):   If $ A $ is square and nonsingular, the relative condition number is bounded by:   $$   \\text{cond}(x) \\leq |A| |A^{-1}|.   $$   Here, $ |A| |A^{-1}| $ is the condition number of the matrix $ A $, which measures the sensitivity of the solution to small perturbations in $ A $ or $ x $.</p> </li> </ul> <p>### 4. Conditioning of Polynomial Zeros</p> <p>Let $ q(x) = x^2 - 2x + 1 $, a quadratic polynomial with a double root at $ x = 1 $. The computational task is to find the roots of $ q(x) $ based on its power basis coefficients \\([1, -2, 1]\\). Here are the key observations:</p> <ul> <li>Perturbation and Root Sensitivity:   If $ q(x) $ is perturbed by a small error $ \\epsilon $, the polynomial becomes $ q(x) + \\epsilon = x^2 - 2x + 1 + \\epsilon $.   The double root at $ x = 1 $ splits into two roots:   $$   x = 1 \\pm \\sqrt{\\epsilon}.   $$</li> </ul> <p>A relative error of $ \\epsilon $ in the coefficients leads to a relative error of $ \\sqrt{\\epsilon} $ in the roots.</p> <ul> <li> <p>Infinite Condition Number:   For small $ \\epsilon $, the roots change dramatically, even for tiny perturbations. Specifically, as $ \\epsilon \\to 0 $, the rate of change of the roots becomes infinite.   The condition number of the coefficients \\([1, -2, 1]\\) for finding the roots is thus infinite.</p> </li> <li> <p>Insight:   The example highlights that polynomial root finding is highly ill-conditioned when the polynomial has multiple or near-multiple roots.   However, strictly speaking, it is the coefficients that are ill-conditioned, not the roots themselves, as the coefficients serve as the input data for this calculation.</p> </li> </ul>"},{"location":"def/MatrixComputation/#5-wilkinson-polynomial","title":"5. Wilkinson Polynomial","text":"<p>The Wilkinson polynomial is defined as: $$ w(x) = (x - 1)(x - 2)\\cdots(x - 20), $$ or equivalently: $$ w(x) = x^{20} - 210x^{19} + 20615x^{18} - \\cdots + 2432902008176640000. $$</p> <ul> <li> <p>Ill-Conditioning of Roots:   Although the roots $ 1, 2, 3, \\ldots, 20 $ are distinct, they are highly sensitive to small changes in the polynomial's coefficients, particularly the coefficient of $ x^{19} $.</p> </li> <li> <p>Perturbation Example:   Perturb the $ x^{19} $-coefficient from $ -210 $ to $ -210 - 2^{-23} $ ($ \\approx -210 - 1.12 \\times 10^{-7} $).   This small change causes drastic shifts in some roots:</p> </li> <li> <p>Roots $ 16 $ and $ 17 $ shift to a complex conjugate pair approximately equal to:     $$     16.73 \\pm 2.81i.     $$</p> </li> <li> <p>Condition Numbers of Perturbed Roots:   For the root near $ 16 $ (denoted $ P_{16}(z) $) and the root near $ 17 $ ($ P_{17}(z) $), the condition numbers with respect to the perturbed coefficient $ z = 210 $ are:   $$   \\text{cond}{16}(210) \\approx 3 \\times 10^{10}, \\quad \\text{cond}{17}(210) \\approx 2 \\times 10^{10}.   $$</p> </li> <li> <p>Asymptotic Region Failure:   The condition numbers are so large that even a perturbation as small as $ 2^{-23} $ falls outside the asymptotic region where the higher-order terms $ o(\\delta z / z) $ in Fact 2 can be neglected.</p> </li> </ul>"},{"location":"def/MatrixComputation/#insights","title":"Insights:","text":"<ol> <li>Polynomial root finding from power basis coefficients is inherently ill-conditioned, especially for polynomials with closely spaced, multiple, or near-multiple roots.</li> <li>In practice, numerical algorithms must take such sensitivities into account, often using alternative representations (e.g., Chebyshev polynomials or orthogonal bases) to mitigate ill-conditioning.</li> </ol>"},{"location":"def/MatrixComputation/#numerical-stability-and-instability","title":"Numerical Stability and Instability","text":"<p>Numerical stability is a crucial concept in numerical analysis, concerning how well an algorithm handles errors introduced during computation, such as rounding or truncation errors. Here's an outline based on the definitions provided:</p>"},{"location":"def/MatrixComputation/#1-definitions","title":"1. Definitions","text":""},{"location":"def/MatrixComputation/#forward-error","title":"Forward Error","text":"<ul> <li>Definition: The difference between the exact function evaluation $ f(x) $ and the perturbed function evaluation $ \\hat{f}(x) $:   $$   \\text{Forward error} = f(x) - \\hat{f}(x).   $$</li> </ul>"},{"location":"def/MatrixComputation/#backward-error","title":"Backward Error","text":"<ul> <li>Definition: A vector $ e \\in \\mathbb{R}^n $ of the smallest norm for which:   $$   f(x + e) = \\hat{f}(x).   $$   If no such $ e $ exists, the backward error is undefined.</li> <li>Interpretation: The backward error measures how far the input $ x $ would need to be perturbed to make the perturbed output $ \\hat{f}(x) $ an exact evaluation.</li> </ul>"},{"location":"def/MatrixComputation/#2-numerical-stability","title":"2. Numerical Stability","text":""},{"location":"def/MatrixComputation/#forward-stability","title":"Forward Stability","text":"<ul> <li>Definition: An algorithm is forward stable if the forward relative error is small for all valid inputs $ x $:   $$   \\frac{|f(x) - \\hat{f}(x)|}{|f(x)|} \\leq \\epsilon,   $$   where $ \\epsilon $ is a modest multiple of the unit roundoff error or truncation error.</li> <li>Interpretation: The computed solution is close to the exact solution, within the error limits dictated by the precision of the input.</li> </ul>"},{"location":"def/MatrixComputation/#backward-stability","title":"Backward Stability","text":"<ul> <li>Definition: An algorithm is backward stable if the backward error exists and satisfies:   $$   \\frac{|e|}{|x|} \\leq \\epsilon,   $$   for all valid inputs $ x $, where $ \\epsilon $ is small.</li> <li>Interpretation: A backward stable algorithm effectively computes the exact solution to a nearby problem. The perturbation in the input is proportional to the inherent errors in the computation.</li> </ul>"},{"location":"def/MatrixComputation/#strong-stability","title":"Strong Stability:","text":"<p>Backward stability is sometimes referred to as strong stability due to its rigorous error guarantees.</p>"},{"location":"def/MatrixComputation/#3-practical-implications","title":"3. Practical Implications","text":"<ul> <li>Stable Algorithms: Produce results that are as accurate as the input data allows. Errors introduced during computation do not grow disproportionately.</li> <li>Unstable Algorithms: Amplify rounding or truncation errors, potentially leading to results that are much less accurate than the errors in the input data would suggest.</li> </ul>"},{"location":"def/MatrixComputation/#4-examples","title":"4. Examples","text":"<ul> <li>Forward Stable Algorithms: Gaussian elimination with partial pivoting is forward stable for most practical problems.</li> <li>Backward Stable Algorithms: Many algorithms for solving linear systems or eigenvalue problems (e.g., LU decomposition or QR decomposition) are designed to be backward stable, ensuring numerical robustness.</li> </ul>"},{"location":"def/MatrixComputation/#5-error-size","title":"5. Error Size","text":"<p>The term \"small\" in these definitions usually means a modest multiple of the size of the input errors, often determined by the machine precision or rounding unit.</p>"},{"location":"def/MatrixComputation/#summary","title":"Summary","text":"<p>Numerical stability ensures that computational errors introduced during algorithm execution do not significantly degrade the accuracy of the results. Forward stability focuses on small output errors, while backward stability ensures that the computed result corresponds to the exact solution of a slightly perturbed problem.</p>"},{"location":"def/MatrixComputation/#weak-numerical-stability-vs-numerical-stability","title":"Weak Numerical Stability vs Numerical Stability","text":"<p>Numerical stability in algorithms characterizes how errors from rounding and truncation affect the accuracy of results. Here's a detailed breakdown based on the provided definitions:</p>"},{"location":"def/MatrixComputation/#1-weak-numerical-stability","title":"1. Weak Numerical Stability","text":""},{"location":"def/MatrixComputation/#definition","title":"Definition:","text":"<p>A numerical algorithm is weakly numerically stable if rounding and truncation errors cause it to evaluate a perturbed function $ \\hat{f}(x) $ satisfying: $$ \\frac{|f(x) - \\hat{f}(x)|}{|f(x)|} \\leq \\epsilon \\cdot \\text{cond}(f(x)), $$ for all valid inputs $ x $.</p>"},{"location":"def/MatrixComputation/#key-characteristics","title":"Key Characteristics:","text":"<ul> <li>The forward error is proportional to the condition number of the input $ x $ and a small error factor $ \\epsilon $ (such as the machine precision).</li> <li>Forward Error Bound: The error magnitude is no worse than what would result from perturbing the data by a small multiple of the unit roundoff.</li> <li>No Backward Error Guarantee: Weak numerical stability does not ensure the existence of a backward error or that the computed result corresponds to a small perturbation of the input.</li> <li>Weaker Variant: An even weaker form of weak stability guarantees the relative error bound only when the data is well-conditioned.</li> </ul>"},{"location":"def/MatrixComputation/#implications","title":"Implications:","text":"<p>Weak numerical stability allows for larger errors in ill-conditioned inputs, where the condition number is high. This makes the method less robust in general compared to numerically stable algorithms.</p>"},{"location":"def/MatrixComputation/#2-numerical-stability_1","title":"2. Numerical Stability","text":""},{"location":"def/MatrixComputation/#definition_1","title":"Definition:","text":"<p>A numerical algorithm is numerically stable if rounding and truncation errors cause it to evaluate a perturbed function $ \\hat{f}(x) $ satisfying: $$ \\frac{|f(x + e) - \\hat{f}(x)|}{|f(x)|} \\leq \\epsilon, $$ where $ e $ is a small relative-to-$ x $ backward error such that $ |e| \\leq \\epsilon |x| $.</p>"},{"location":"def/MatrixComputation/#key-characteristics_1","title":"Key Characteristics:","text":"<ul> <li>Backward Error Guarantee: The computed value $ \\hat{f}(x) $ corresponds to the exact function $ f(x) $ evaluated at a nearby data point $ x + e $.</li> <li>Robustness: Numerical stability ensures that the result is consistent with a slight perturbation in the input.</li> <li>Behavior for Ill-Conditioned Inputs: While the forward error can still be large for ill-conditioned inputs (due to high condition numbers), the algorithm remains robust because the backward error is small.</li> </ul>"},{"location":"def/MatrixComputation/#3-comparisons","title":"3. Comparisons","text":"Aspect Weak Numerical Stability Numerical Stability Forward Error Proportional to $ \\epsilon \\cdot \\text{cond}(f(x)) $. Proportional to $ \\epsilon $, independent of condition number. Backward Error No guarantee or may not exist. Guaranteed to exist and be small ($ Error Interpretation May not correspond to small perturbations of the input. Corresponds to a small perturbation of the input. Handling Ill-Conditioned Data Errors can grow with condition number. Backward error remains small; forward error may still be large."},{"location":"def/MatrixComputation/#4-visualization-figure-372","title":"4. Visualization (Figure 37.2)","text":"<ul> <li> <p>Weak Stability:   The computed value $ \\hat{f}(x) $ lies within a larger circle around $ f(x) $, which includes errors consistent with the condition number. The result may not correspond to a small input perturbation.</p> </li> <li> <p>Numerical Stability:   The computed value $ \\hat{f}(x) $ lies near or inside the image of the small perturbations $ x + e $. This implies that the result corresponds to a slightly perturbed input.</p> </li> <li> <p>Backward Stability:   The computed value $ \\hat{f}(x) $ lies entirely within the shaded image of the small perturbations of $ x $.</p> </li> </ul>"},{"location":"def/MatrixComputation/#5-summary","title":"5. Summary","text":"<ul> <li>Weak Stability provides weaker guarantees and is more error-prone, especially for ill-conditioned data, as it allows larger forward errors without requiring backward stability.</li> <li>Numerical Stability ensures more robust performance by tightly linking the computed result to a small perturbation in the input, making it preferable for reliable computations.</li> </ul>"},{"location":"def/MatrixComputation/#key-facts-about-numerical-stability-and-backward-stability","title":"Key Facts about Numerical Stability and Backward Stability","text":""},{"location":"def/MatrixComputation/#1-numerical-stability-and-ill-conditioned-problems","title":"1. Numerical Stability and Ill-Conditioned Problems","text":"<ul> <li>Ill-Conditioned Problems: Even a numerically stable algorithm may yield inaccurate results when applied to an ill-conditioned problem. This is because small errors in the input data (e.g., rounding or measurement errors) can amplify into large errors in the solution due to the problem's inherent sensitivity.</li> <li>No Algorithmic Correction: A numerical algorithm cannot compensate for errors already present in the input data or create information that wasn't implicitly there. For ill-conditioned problems, inaccuracies are often unavoidable.</li> </ul>"},{"location":"def/MatrixComputation/#2-backward-stability","title":"2. Backward Stability","text":"<ul> <li>Perturbation Equivalent: A backward stable algorithm ensures that its rounding and truncation errors are equivalent to solving the problem with a slightly perturbed input. The computed results are realistic and consistent with an exact arithmetic solution for the perturbed data.</li> <li>Negligibility: In most cases, this additional error due to backward instability is negligible compared to the inherent data errors.</li> </ul>"},{"location":"def/MatrixComputation/#3-forward-error-in-backward-stability","title":"3. Forward Error in Backward Stability","text":"<ul> <li>Condition Number Dependency: The forward error in a backward stable algorithm follows the condition number bound (Fact 2, Section 37.4). This means the forward error can grow in proportion to the problem's condition number, especially for ill-conditioned data.</li> </ul>"},{"location":"def/MatrixComputation/#4-examples-of-backward-stable-algorithms","title":"4. Examples of Backward Stable Algorithms","text":"<p>The following are well-known backward stable algorithms and their properties:</p> <ol> <li>Single Floating Point Operations: </li> <li> <p>Any single floating-point operation (e.g., addition, multiplication) is both forward and backward stable.</p> </li> <li> <p>Dot Product Algorithm (Naive): </p> </li> <li> <p>The naive dot product algorithm is backward stable, but not generally forward stable due to potential cancellation of significant digits during summation.</p> </li> <li> <p>Gaussian Elimination: </p> </li> <li>With complete pivoting: Strictly backward stable.  </li> <li> <p>With partial pivoting: Not strictly backward stable, but considered \u201cbackward stable in practice\u201d since instability cases are extraordinarily rare.</p> </li> <li> <p>Triangular Back Substitution: </p> </li> <li>The back-substitution algorithm computes a solution $ \\hat{x} $ such that it solves a nearby system $ (T + E)\\hat{x} = b $, where $ |e_{ij}| \\leq |t_{ij}| $.  </li> <li> <p>Backward Stable: Ensures the computed solution is consistent with a slightly perturbed system.</p> </li> <li> <p>QR Factorization (Householder and Givens Methods): </p> </li> <li> <p>Both methods for $ A = QR $, where $ Q $ is orthogonal and $ R $ is upper triangular, are backward stable.</p> </li> <li> <p>Singular Value Decomposition (SVD): </p> </li> <li> <p>The Golub\u2013Kahan\u2013Reinsch algorithm is backward stable for computing $ A = U \\Sigma V^T $, where $ U $ and $ V $ are orthogonal matrices, and $ \\Sigma $ is diagonal.</p> </li> <li> <p>Least-Squares Problems: </p> </li> <li> <p>The following methods for solving least-squares problems are backward stable:  </p> <ul> <li>Householder QR Factorization.  </li> <li>Givens QR Factorization.  </li> <li>Singular Value Decomposition (SVD).</li> </ul> </li> <li> <p>Eigenvalue Computations: </p> </li> <li>The implicit double-shift QR iteration is backward stable for finding eigenvalues.</li> </ol>"},{"location":"def/MatrixComputation/#practical-implications","title":"Practical Implications","text":"<ul> <li>Selection of Algorithms: Backward stable algorithms are highly preferred in practice because they ensure realistic results for small perturbations, even if forward errors are amplified for ill-conditioned data.</li> <li>Understanding Stability: Recognizing whether an algorithm is backward or forward stable helps predict its behavior in different numerical contexts.</li> </ul>"},{"location":"def/MatrixComputation/#procedures-to-compute-stability-and-conditioning","title":"Procedures to Compute Stability and Conditioning","text":"<p>To analyze the numerical stability of algorithms and the conditioning of computational problems, follow these structured steps:</p>"},{"location":"def/MatrixComputation/#1-compute-problem-conditioning","title":"1. Compute Problem Conditioning","text":""},{"location":"def/MatrixComputation/#definition_2","title":"Definition:","text":"<p>Conditioning measures how sensitive a problem's output is to small perturbations in the input.</p>"},{"location":"def/MatrixComputation/#steps-to-compute-conditioning","title":"Steps to Compute Conditioning","text":"<ol> <li> <p>Define the Function $ f(x) $:    Let $ f : \\mathbb{R}^n \\to \\mathbb{R}^m $ represent the function you are analyzing.</p> </li> <li> <p>Determine Sensitivity:    Compute the condition number:    $$    \\text{cond}_f(x) = \\frac{|D(x)| \\cdot |x|}{|f(x)|}    $$    where $ D(x) $ is the derivative (or Jacobian) of $ f(x) $.</p> </li> <li> <p>Interpret Results:</p> </li> <li>If $ \\text{cond}_f(x) $ is large, the problem is ill-conditioned, meaning small input errors may lead to large output errors.</li> <li>If $ \\text{cond}_f(x) $ is small, the problem is well-conditioned.</li> </ol>"},{"location":"def/MatrixComputation/#example-polynomial-root-finding","title":"Example: Polynomial Root Finding","text":"<p>For $ q(x) = x^2 - 2x + 1 $: - Double root at $ x = 1 $. - Perturbation $ \\epsilon $ in coefficients causes root error $ \\sqrt{\\epsilon} $, leading to infinite condition number for the coefficients near multiple roots.</p>"},{"location":"def/MatrixComputation/#2-assess-algorithm-stability","title":"2. Assess Algorithm Stability","text":""},{"location":"def/MatrixComputation/#definition_3","title":"Definition:","text":"<p>Stability measures how rounding and truncation errors affect the accuracy of the algorithm's result.</p>"},{"location":"def/MatrixComputation/#steps-to-compute-algorithm-stability","title":"Steps to Compute Algorithm Stability","text":"<ol> <li>Forward Error Analysis:    Evaluate the forward error:    $$    \\text{Forward Error} = \\frac{|f(x) - \\hat{f}(x)|}{|f(x)|}    $$</li> <li> <p>If the error is small for all inputs, the algorithm is forward stable.</p> </li> <li> <p>Backward Error Analysis:    Compute the smallest perturbation $ e $ such that:    $$    f(x + e) = \\hat{f}(x)    $$</p> </li> <li> <p>If $ |e| \\leq \\epsilon |x| $, the algorithm is backward stable.</p> </li> <li> <p>Weak Stability:    If rounding errors satisfy:    $$    \\frac{|f(x) - \\hat{f}(x)|}{|f(x)|} \\leq \\epsilon \\cdot \\text{cond}_f(x)    $$    the algorithm is weakly stable.</p> </li> </ol>"},{"location":"def/MatrixComputation/#example-naive-dot-product","title":"Example: Naive Dot Product","text":"<p>Given $ x = [a, b] $ and $ P(x) = a - b $: - For $ a \\approx b $, significant digit cancellation may occur. - Backward stability holds, but forward stability fails because the subtraction magnifies relative errors.</p>"},{"location":"def/MatrixComputation/#3-numerical-conditioning-of-matrix-algorithms","title":"3. Numerical Conditioning of Matrix Algorithms","text":""},{"location":"def/MatrixComputation/#matrix-multiplication-conditioning","title":"Matrix Multiplication Conditioning:","text":"<p>Condition number for $ A x $ is given by: $$ \\text{cond}(x) = \\frac{|A| \\cdot |x|}{|Ax|} $$ For nonsingular $ A $: $$ \\text{cond}(x) \\leq |A| \\cdot |A^{-1}| $$</p>"},{"location":"def/MatrixComputation/#example-gaussian-elimination","title":"Example: Gaussian Elimination","text":"<ul> <li>Complete pivoting ensures backward stability.</li> <li>Partial pivoting may not be strictly backward stable but is considered stable in practice.</li> </ul>"},{"location":"def/MatrixComputation/#4-numerical-stability-in-least-squares-problems","title":"4. Numerical Stability in Least Squares Problems","text":""},{"location":"def/MatrixComputation/#method","title":"Method:","text":"<ul> <li>Solve $ Ax = b $ using QR factorization or SVD.</li> <li>Stability depends on:</li> <li>Orthogonal $ Q $ properties in QR decomposition.</li> <li>Singular values of $ A $ in SVD.</li> </ul>"},{"location":"def/MatrixComputation/#example-qr-factorization","title":"Example: QR Factorization","text":"<p>Given $ A = QR $, where $ Q $ is orthogonal and $ R $ is upper triangular: - Both Householder and Givens methods are backward stable.</p>"},{"location":"def/MatrixComputation/#5-eigenvalue-problems","title":"5. Eigenvalue Problems","text":""},{"location":"def/MatrixComputation/#method_1","title":"Method:","text":"<ul> <li>Compute eigenvalues using iterative techniques like QR iterations.</li> </ul>"},{"location":"def/MatrixComputation/#example-wilkinson-polynomial","title":"Example: Wilkinson Polynomial","text":"<p>Perturbing the $ x^{19} $ coefficient in $ w(x) = (x-1)(x-2)...(x-20) $ drastically changes eigenvalues. Condition numbers $ \\text{cond}{16}(210) \\approx 3 \\times 10^{10} $ and $ \\text{cond}{17}(210) \\approx 2 \\times 10^{10} $ illustrate ill-conditioning.</p>"},{"location":"def/MatrixComputation/#conclusion","title":"Conclusion","text":"<p>To assess stability and conditioning: 1. Compute condition numbers to measure problem sensitivity. 2. Evaluate forward and backward errors for algorithm stability. 3. Analyze examples such as matrix operations, polynomial roots, or iterative eigenvalue computations to understand practical implications.</p>"},{"location":"def/MatrixComputation/#examples-of-numerical-stability-and-conditioning","title":"Examples of Numerical Stability and Conditioning","text":""},{"location":"def/MatrixComputation/#1-forward-stable-but-not-backward-stable-outer-product-algorithm","title":"1. Forward Stable but Not Backward Stable: Outer Product Algorithm","text":"<p>Example: Compute $ A = xy^T $, where $ x, y \\in \\mathbb{R}^n $. - Forward Stability:   - The computed $ A $ (outer product of $ x $ and $ y $) is correctly rounded, ensuring the forward relative error is small.   - Example: If $ x = [1, 2] $, $ y = [3, 4] $, then $ A = \\begin{bmatrix} 3 &amp; 4 \\ 6 &amp; 8 \\end{bmatrix} $. - Backward Instability:   - Rounding errors perturb $ A $ into a matrix of higher rank, making it impossible to represent $ A $ as $ xy^T $ for perturbed $ x $ and $ y $.</p>"},{"location":"def/MatrixComputation/#2-backward-vs-forward-errors-taylor-series-approximation","title":"2. Backward vs. Forward Errors: Taylor Series Approximation","text":"<p>Example: Evaluate $ f(x) = e^x $ at $ x = 1 $ using the Taylor series approximation: $$ \\hat{f}(x) = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!}. $$ - Forward Error:   $$   \\text{Forward Error} = |f(1) - \\hat{f}(1)| \\approx 2.7183 - 2.6667 = 0.0516.   $$ - Backward Error:   Solve for $ y $ such that $ f(y) = \\hat{f}(1) $. The backward error is:   $$   1 - \\ln(\\hat{f}(1)) \\approx 1 - \\ln(2.6667) = 0.0192.   $$</p>"},{"location":"def/MatrixComputation/#3-numerically-unstable-algorithm-logarithm-near-zero","title":"3. Numerically Unstable Algorithm: Logarithm Near Zero","text":"<p>Example: Evaluate $ f(x) = \\ln(1 + x) $ for $ x \\approx 0 $ using $ \\text{fl}(\\ln(1 \\oplus x)) $: - For $ x = 10^{-16} $ in 16-digit arithmetic:   - $ \\ln(1 \\oplus 10^{-16}) $ is computed as $ \\ln(1) = 0 $ due to rounding.   - Exact value $ f(x) = 10^{-16} $, so relative error = 100%.</p> <ul> <li>Analysis:</li> <li>The function is well-conditioned ($ \\text{cond}_f(x) = 1 $ as $ x \\to 0 $), but the algorithm is numerically unstable due to precision loss.</li> </ul>"},{"location":"def/MatrixComputation/#4-improved-logarithm-algorithm-taylor-series","title":"4. Improved Logarithm Algorithm: Taylor Series","text":"<p>Example: Use the Taylor expansion: $$ f(x) = \\ln(1 + x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\cdots. $$ - Advantages:   - For $ x \\approx 0 $, higher precision is achieved with a few terms. - Limitations:   - Converges slowly for $ |x| \\approx 1 $.   - Alternative algorithms like $ \\text{fl}(\\ln(1 \\oplus x)) $ may be required for $ |x| &gt; 1 $.</p>"},{"location":"def/MatrixComputation/#5-gaussian-elimination-without-pivoting","title":"5. Gaussian Elimination Without Pivoting","text":"<p>Example: Solve: $$ \\begin{aligned} 10^{-10}x_1 + x_2 &amp;= 1, \\ x_1 + 2x_2 &amp;= 3, \\end{aligned} $$ using 9-digit arithmetic. - Steps:   - Eliminate $ x_1 $:     $$     x_2 = 1, \\quad x_1 = 0.     $$   - Exact solution: $ x_1 = 1 - 2 \\cdot 10^{-10}, \\; x_2 = 1 - 3 \\cdot 10^{-10} \\(.   - **Analysis:**   - Large error is due to numerical instability, not ill-conditioning (\\) \\kappa(A) \\approx 9 $).</p>"},{"location":"def/MatrixComputation/#6-unstable-algorithm-for-eigenvalue-computations","title":"6. Unstable Algorithm for Eigenvalue Computations","text":"<p>Example: Compute eigenvalues of $ A = \\text{diag}(1, 2, 3, \\ldots, 20) $ by finding roots of the characteristic polynomial. - Problem:   - The Wilkinson polynomial has highly ill-conditioned roots, even though the eigenvalues themselves are well-conditioned ($ \\kappa \\leq |E|_F $ for perturbation $ E $).   - Transformation to companion form introduces instability.  </p>"},{"location":"def/MatrixComputation/#7-alternative-eigenvalue-algorithms","title":"7. Alternative Eigenvalue Algorithms","text":"<p>Use iterative QR methods to avoid polynomial root finding instability: - Implicit double-shift QR iteration is backward stable. - It directly computes eigenvalues without forming ill-conditioned polynomials.</p>"},{"location":"def/MatrixComputation/#conclusion_1","title":"Conclusion","text":"<ul> <li>Numerical stability and conditioning depend on both the algorithm and the problem.  </li> <li>Forward stability focuses on relative error in results, while backward stability ensures results align with a slightly perturbed input.  </li> <li>Examples highlight the importance of carefully choosing stable algorithms for ill-conditioned problems.</li> </ul>"},{"location":"frameworks/pytorch/ch1pytorchIntro/","title":"PyTorch Introduction","text":"<p>What is PyTorch ?</p> <ol> <li>Open source machine learning library</li> <li>Developed by Facebook'sAI Research Lab</li> <li>It leverages the power of GPU</li> <li>It uses dynamic computational graphs for automatic computation of gradients</li> <li>It makes easier to test and develop new ideas like NNs , ML algorithms and do implement, test our algorithms.</li> </ol> <p>What are the other libraries than PyTorch for doing same tasks ?</p> <ol> <li>CNTK Microsoft</li> <li>Caffe and updated version Caffe2</li> <li>TensorFlow and it's updated versions</li> <li>Keras, Theano, MXNET, dy/net , chainer, GLUON</li> </ol> <p>How to represent the data in the PyTorch ?</p> <p>In NumPy, we have arrays as data structure to store the data inside memory similarly PyTorch has Tensors to store the data inside memory.</p> <p>Why Tensors are importent in PyTorch ?</p> <p>Tensors can directly run on GPU for faster computation and also on CPU but numpy arrays can't be run onto the GPU only runnable onto the CPU.</p> <p>PyTorch has features to create random matrices, zero values matrices, directly from data or  for the models weight intialization</p> <p>There are the different operation we can perform onto the tensors like addition, indexing, resizing tensor to a particular size using view function.</p> <pre><code>import torch\n\n# rand function returns random number of given tensor size\nx = torch.rand(2, 3) # 2 x 3\ny = torch.rand(3,3) # 3x 3\n\n# all zeros\nx = torch.zeros(5, 3)\n\n# given data to tensor\ndata = [4.5 , 53.4, 8.3]\nx = torch.tensor(data)\n\n# size of tensor importent in designing large tensor\nprint(x.size())\n\n# operations\nx = torch.rand(3, 3) # 3 x 3\ny = torch.rand(3,3) # 3x 3\nprint(torch.add(x, y))\n\n# accessing particular set of element from starts to end in the tensor\nprint(x[:, 1)\n\n# resize the tensor to a particular size using view function\nx = torch.randn(4, 4)\ny = x.view(16) # flattening, usefull with nns\nz = x.view(-1, 8) # dynamic dimention, to define the num of patches , dim that we define for the our # input data varies then we can't hardcode the reszing factors\n\nprint(x.size())\nprint(y.size())\nprint(z.size())\n\nx = torch.randn(1, 4, 32, 24)\ny = x.view(8, 2, -1, 3, 8)\nprint(y.size())\n</code></pre> <p>NumPy array vs Torch tensors</p> <p>arrays are desined for CPU only while the Tensor can run on CPU as well as GPU.</p> <p>Matrix Multiplication in PyTorch used in many NNs implementations</p> <pre><code># tensor to numpy arrays\na = torch.ones(5)\nb = a.numpy()\n\n# numpy array to tensor\na = numpy.ones(5)\nb = torch.from_numpy(a)\n\n# matrix multiplication\nimport torch\nM1 = torch.randn(2, 3)\nM2 = torch.randn(3,3)\n\nresult = torch.mm(M1, M2)\nprint(result.size())\n\n# Batch Matrix multiplication\nimport torch\nB1 = torch.randn(10, 3, 4) # 10 , 3x4 matrices\nB2 = torch.randn(10, 4, 5) # 10 , 4 x 5 matrices batch\nresult = torch.bmm(B1, B2)\nprint(result.size()) # 10, (3, 5)\n\n# Tensor Concatenation\nresult = torch.cat(T1, T2)\n\n# reduce or increate the dimensions of the tensors\ntorch.sqeueeze\ntorch.unsqeueeze\n# change tensor dimensions and many more\n</code></pre> <p>check the docs : https://docs.pytorch.org/docs/stable/torch.html</p> <p>Computational Graphs : They actually defines the neural network opeations with given inputs.</p> <pre><code>flowchart TD\n    X1(X) --&gt;|a| a1[a]\n    a1 --&gt; Y1(Y)\n    X1 --&gt; W1(W)\n    Y1 --&gt; B1(b)\n    B1 --&gt; Y1\n</code></pre> <p>and </p> <p>Automatic Gradient Computation compute the gradient w.r.t to all the parameters</p> <pre><code>import torch\nx = torch.ones(2, 2)\ny = torch.ones(2, 1)\n\n# we compute the gradient of these params and update them thus set true.\nw = torch.randn(2, 1, requires_grad=True)\nb = torch.randn(1, requires_grad=True)\n\n# prediction\np = torch.sigmoid(torch.mm(x, w) + b)\n\n# cross entropy loss\nloss = -y*torch.log(p) - (1-y)*torch.log(1-p)\n\n# cost to minimize it \ncost = loss.mean()\n\n# compute gradient\ncost.backward()\n# we can see \nprint(w.grad)\nprint(b.grad)\n</code></pre> <p>Training Procedure of neural networks 1. Define the Neural networks 2. Iterate over a dataset of inputs 3. Process input through the network 4. Compute the loss 5. Propagate the gradients back into the network's parameters 6. Update the weights of the networks.</p> <ol> <li>Building Neural Network using PyTorch</li> </ol> <p>How to define a Neural Network class ?</p> <pre><code>import torch\nimport torch.nn as nn\n\n# nn.Module class class is the super class of the NNet class\n# means NNet class inherits the properties from the nn.Module\nclass NNet(nn.Module):\n    def __init__(self):\n        super(NNet, self).__init__()\n        # create the layers which will be needed to define the forward pass of the nns\n\n    def forward(self, x):\n        # define the feed-forward pass of the nns\n\n# we don't needed to define the backward pass into the our NNet class.\n</code></pre> <p>WE will defin CNN for MNIST</p> <p></p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Depth : 1 input image channel, filter size : 6 output channels, grad scale image : 5x5 square : convolution kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n        # we don't know what batch size we are apply to these tensor thus we just flatten the tensor\n        # because convolution is the trasnformation from volume to volume but the fully connected layers are features to features\n        # we can also use the flatten function also\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n</code></pre> <ol> <li>Iterate over the dataset of Input</li> <li>Images : Pillow, OpenCV are usefull</li> <li>Audio : Scipy and librosa</li> <li>Text : NLTK and SpaCy</li> </ol> <p>Loading data into the memory as numpy array then convert into the tensor for GPU if we have access to it.</p> <p>Loading Image data - with torchvision torchvision is extremly easy to load the exisiting benchmarkding datasets.</p> <pre><code># other than hyperparameter tuning, and inferencing\n# we do most of time preprocessing, transforming, feature # engineering dataset for the our nn model.\n\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n</code></pre> <p>Process the input through the network via feed forward functions</p> <p>Then compute the loss Loss function A loss function takes the (output, target) pair of inputs.</p> <p>Computes a value that estimates how far away the output is from the target.</p> <p>There are several different loss functions under the nn package.</p> <p>A simple loss can be:</p> <p>nn.MSELoss</p> <p>It computes the mean-squared error between the input and the target</p> <pre><code>output = net(output)\ntarget = torch.randn(10)\n# a dummy target ,for example \ntarget = target.view(1, -1)\n#make it same shape as output\ncriterion = nn.MSELoss()\nloss = criterion(output, target)\n</code></pre> <p>Then Propagate the gradient back to the networks parameters</p> <p>and upate the weights of the networks</p> <pre><code>import torch.optim as optim\n\n# create our optimizer\n\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n# in our training loop\noptimizer.zero_grad()\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step() #update the params\n</code></pre> <p>Total training sample : 80000, batch_size = 50 then each iteration takes 30secs then how many hours of 3 epoch training ?</p> <p>One epoch : iterate through all the training set of the all batches</p>"},{"location":"notebook/Scipy-Optimize/","title":"Optimization using Scipy i.e <code>scipy.optimize</code>","text":"<pre><code># set up the development enviroments\nimport numpy as np\nimport scipy.linalg as la\nimport scipy.optimize as opt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre>"},{"location":"notebook/Scipy-Optimize/#1-single-variable-functions","title":"1. Single Variable Functions","text":"<pre><code>f = lambda x: x**2\nprint(\"f(89):\", f(89))\n# initialize the value of x in certain range \nx = np.linspace(-2, 2, 500)\nplt.plot(f(x), x)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(\"f(x) = $x^{2} + 9x + 1$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>No handles with labels found to put in legend.\n\n\nf(89): 7921\n</code></pre> <pre><code># finding the minima of this curve\n# Objective function. Scalar function, must return a scalar.\n# Local minimization of scalar function of one variable.\nmin = opt.minimize_scalar(f)\nmin\n</code></pre> <pre><code> message: \n          Optimization terminated successfully;\n          The returned value satisfies the termination criteria\n          (using xtol = 1.48e-08 )\n success: True\n     fun: 0.0\n       x: 0.0\n     nit: 4\n    nfev: 7\n</code></pre> <pre><code>x = np.linspace(-2, 2, 500)\nplt.plot(f(x), x)\nplt.scatter([min.x], [min.fun], label='solution')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(\"f(x) = $x^{2} + 9x + 1$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># let's add constraints x &gt;= 1 then \nsol = opt.minimize_scalar(f, bounds=(1, 1000), method='bounded')\nx = np.linspace(-5,5,500)\nplt.plot(x, f(x))\nplt.scatter([sol.x], [sol.fun], label='solution')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>f = lambda x : (x - 2)**2/3 * (x + 2)**2/3 - x**(1.3)\n\nprint(f(393))\n\nx = np.linspace(-5, 5, 500)\nplt.plot(x, f(x))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('f(x) = $(x - 2)^{2/3} (x + 2)^{2/3} - x^{1.3}$')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>&lt;ipython-input-37-1df52e6ecde4&gt;:1: RuntimeWarning: invalid value encountered in power\n  f = lambda x : (x - 2)**2/3 * (x + 2)**2/3 - x**(1.3)\nNo handles with labels found to put in legend.\n\n\n2650359643.8735137\n</code></pre> <pre><code>sol = opt.minimize_scalar(f)\nx = np.linspace(-5, 5, 500)\nprint([sol.x])\nplt.scatter([sol.x], [sol.fun], label='solution')\nplt.plot(x, f(x))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('f(x) = $(x - 2)^{2/3} (x + 2)^{2/3} - x^{1.3}$')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>[2.3665310514335505]\n\n\n&lt;ipython-input-37-1df52e6ecde4&gt;:1: RuntimeWarning: invalid value encountered in power\n  f = lambda x : (x - 2)**2/3 * (x + 2)**2/3 - x**(1.3)\n</code></pre> <pre><code># let's add constraints x &gt;= 1 to see the solutions\nsol = opt.minimize_scalar(f, bounds=(1, 1000), method='bounded')\nx = np.linspace(-5,5,500)\nplt.plot(x, f(x))\nplt.scatter([sol.x], [sol.fun], label='solution')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>&lt;ipython-input-37-1df52e6ecde4&gt;:1: RuntimeWarning: invalid value encountered in power\n  f = lambda x : (x - 2)**2/3 * (x + 2)**2/3 - x**(1.3)\n</code></pre>"},{"location":"notebook/Scipy-Optimize/#2-multi-variable-functions","title":"2. Multi Variable Functions","text":"<p>The multivariate quadratics form \\(x^T A x + b^T x + c\\) where \\(x \\in R^n\\) and \\(A \\in R^{mxn}\\)</p> <pre><code>n = 2 # number of independent variables\nA = np.random.randn(n,n) # generate a random matrix of dim 3x2 \nb = np.random.randn(n)\n\nf = lambda x: np.dot(x-b, A @ (x-b))\n\nsol = opt.minimize(f, np.zeros(n))\nsol\n</code></pre> <pre><code>  message: Desired error not necessarily achieved due to precision loss.\n  success: False\n   status: 2\n      fun: -283076.52934196056\n        x: [ 9.642e+02  5.060e+01]\n      nit: 1\n      jac: [-6.971e+02  2.113e+03]\n hess_inv: [[ 1.142e+01  4.219e+00]\n            [ 4.219e+00  1.414e+00]]\n     nfev: 348\n     njev: 112\n</code></pre> <pre><code>print(sol.x, sol.fun)\n</code></pre> <pre><code>[964.15932083  50.60410118] -283076.52934196056\n</code></pre> <pre><code>print(b)\nprint(la.norm(sol.x -b))\n</code></pre> <pre><code>[0.19366828 0.49468862]\n965.2671819409132\n</code></pre> <pre><code># rough idea we can increase the number of independent variables upto certain higher dim\nn = 100\nA = np.random.randn(n+1,n)\nA = A.T @ A\n\nb = np.random.rand(n)\n\nf = lambda x : np.dot(x - b, A @ (x - b)) # (x - b)^T A (x - b)\n\nsol = opt.minimize(f, np.zeros(n))\nprint(sol)\nprint(la.norm(sol.x - b))\n</code></pre> <pre><code>  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 1.627401352740875e-10\n        x: [ 1.574e-01  4.310e-01 ...  1.415e-01  6.544e-01]\n      nit: 122\n      jac: [-3.629e-08 -1.181e-06 ...  4.563e-07 -7.903e-07]\n hess_inv: [[ 1.004e+01 -1.500e+00 ...  6.227e+00 -3.190e+00]\n            [-1.500e+00  2.959e-01 ... -1.036e+00  5.297e-01]\n            ...\n            [ 6.227e+00 -1.036e+00 ...  4.987e+00 -1.846e+00]\n            [-3.190e+00  5.297e-01 ... -1.846e+00  1.233e+00]]\n     nfev: 13130\n     njev: 130\n0.00030993826439077414\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebook/lab-Scipy-Optimize/","title":"Optimization using Scipy i.e <code>scipy.optimize</code>","text":"<pre><code># set up the development enviroments\nimport numpy as np\nimport scipy.linalg as la\nimport scipy.optimize as opt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre>"},{"location":"notebook/lab-Scipy-Optimize/#1-single-variable-functions","title":"1. Single Variable Functions","text":"<pre><code>f = lambda x: x**2\nprint(\"f(89):\", f(89))\n# initialize the value of x in certain range \nx = np.linspace(-2, 2, 500)\nplt.plot(f(x), x)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(\"f(x) = $x^{2} + 9x + 1$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>No handles with labels found to put in legend.\n\n\nf(89): 7921\n</code></pre> <pre><code># finding the minima of this curve\n# Objective function. Scalar function, must return a scalar.\n# Local minimization of scalar function of one variable.\nmin = opt.minimize_scalar(f)\nmin\n</code></pre> <pre><code> message: \n          Optimization terminated successfully;\n          The returned value satisfies the termination criteria\n          (using xtol = 1.48e-08 )\n success: True\n     fun: 0.0\n       x: 0.0\n     nit: 4\n    nfev: 7\n</code></pre> <pre><code>x = np.linspace(-2, 2, 500)\nplt.plot(f(x), x)\nplt.scatter([min.x], [min.fun], label='solution')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(\"f(x) = $x^{2} + 9x + 1$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># let's add constraints x &gt;= 1 then \nsol = opt.minimize_scalar(f, bounds=(1, 1000), method='bounded')\nx = np.linspace(-5,5,500)\nplt.plot(x, f(x))\nplt.scatter([sol.x], [sol.fun], label='solution')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>f = lambda x : (x - 2)**2/3 * (x + 2)**2/3 - x**(1.3)\n\nprint(f(393))\n\nx = np.linspace(-5, 5, 500)\nplt.plot(x, f(x))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('f(x) = $(x - 2)^{2/3} (x + 2)^{2/3} - x^{1.3}$')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>&lt;ipython-input-37-1df52e6ecde4&gt;:1: RuntimeWarning: invalid value encountered in power\n  f = lambda x : (x - 2)**2/3 * (x + 2)**2/3 - x**(1.3)\nNo handles with labels found to put in legend.\n\n\n2650359643.8735137\n</code></pre> <pre><code>sol = opt.minimize_scalar(f)\nx = np.linspace(-5, 5, 500)\nprint([sol.x])\nplt.scatter([sol.x], [sol.fun], label='solution')\nplt.plot(x, f(x))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('f(x) = $(x - 2)^{2/3} (x + 2)^{2/3} - x^{1.3}$')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>[2.3665310514335505]\n\n\n&lt;ipython-input-37-1df52e6ecde4&gt;:1: RuntimeWarning: invalid value encountered in power\n  f = lambda x : (x - 2)**2/3 * (x + 2)**2/3 - x**(1.3)\n</code></pre> <pre><code># let's add constraints x &gt;= 1 to see the solutions\nsol = opt.minimize_scalar(f, bounds=(1, 1000), method='bounded')\nx = np.linspace(-5,5,500)\nplt.plot(x, f(x))\nplt.scatter([sol.x], [sol.fun], label='solution')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>&lt;ipython-input-37-1df52e6ecde4&gt;:1: RuntimeWarning: invalid value encountered in power\n  f = lambda x : (x - 2)**2/3 * (x + 2)**2/3 - x**(1.3)\n</code></pre>"},{"location":"notebook/lab-Scipy-Optimize/#2-multi-variable-functions","title":"2. Multi Variable Functions","text":"<p>The multivariate quadratics form \\(x^T A x + b^T x + c\\) where \\(x \\in R^n\\) and \\(A \\in R^{mxn}\\)</p> <pre><code>n = 2 # number of independent variables\nA = np.random.randn(n,n) # generate a random matrix of dim 3x2 \nb = np.random.randn(n)\n\nf = lambda x: np.dot(x-b, A @ (x-b))\n\nsol = opt.minimize(f, np.zeros(n))\nsol\n</code></pre> <pre><code>  message: Desired error not necessarily achieved due to precision loss.\n  success: False\n   status: 2\n      fun: -283076.52934196056\n        x: [ 9.642e+02  5.060e+01]\n      nit: 1\n      jac: [-6.971e+02  2.113e+03]\n hess_inv: [[ 1.142e+01  4.219e+00]\n            [ 4.219e+00  1.414e+00]]\n     nfev: 348\n     njev: 112\n</code></pre> <pre><code>print(sol.x, sol.fun)\n</code></pre> <pre><code>[964.15932083  50.60410118] -283076.52934196056\n</code></pre> <pre><code>print(b)\nprint(la.norm(sol.x -b))\n</code></pre> <pre><code>[0.19366828 0.49468862]\n965.2671819409132\n</code></pre> <pre><code># rough idea we can increase the number of independent variables upto certain higher dim\nn = 100\nA = np.random.randn(n+1,n)\nA = A.T @ A\n\nb = np.random.rand(n)\n\nf = lambda x : np.dot(x - b, A @ (x - b)) # (x - b)^T A (x - b)\n\nsol = opt.minimize(f, np.zeros(n))\nprint(sol)\nprint(la.norm(sol.x - b))\n</code></pre> <pre><code>  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 1.627401352740875e-10\n        x: [ 1.574e-01  4.310e-01 ...  1.415e-01  6.544e-01]\n      nit: 122\n      jac: [-3.629e-08 -1.181e-06 ...  4.563e-07 -7.903e-07]\n hess_inv: [[ 1.004e+01 -1.500e+00 ...  6.227e+00 -3.190e+00]\n            [-1.500e+00  2.959e-01 ... -1.036e+00  5.297e-01]\n            ...\n            [ 6.227e+00 -1.036e+00 ...  4.987e+00 -1.846e+00]\n            [-3.190e+00  5.297e-01 ... -1.846e+00  1.233e+00]]\n     nfev: 13130\n     njev: 130\n0.00030993826439077414\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebook/lab-peft-from-scratch/","title":"Lab peft from scratch","text":"PEFT from Scratch <p>Questions 1. How to implement different PEFT approaches ? 2. How to update these large pre-trained models with low resources  ? 3. How to apply PEFT methods to large language models for optimization of the model performance efficiently ?</p>"},{"location":"notebook/lab-peft-from-scratch/#setup-and-requirements","title":"Setup and Requirements","text":"<p>We will use the pretrained model t5-small or autoregressive models like gpt-neo-125M for running our experimentation faster.</p> <pre><code>import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt \nimport time \nfrom tqdm.notebook import tqdm \nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive, fixed\nfrom IPython.display import display, HTML\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport torch\n\nfrom transformers import AutoModelForSequenceClassification, AutoModelForCausalLM\nfrom transformers import  AutoTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset\nimport datasets\nfrom peft import get_peft_model, LoraConfig, TaskType, PrefixTuningConfig, PromptEncoderConfig\n</code></pre> <pre><code># parameters \nrandom_seed = 42\ntorch.manual_seed(random_seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nmodel = \"google-t5/t5-small\"\ncausal_model = \"EleutherAI/gpt-neo-125m\"\n\ndataset = \"stanfordnlp/imdb\"\n\nepochs = 3\nbatch_size = 8\nlearning_rate = 1e-4\n\nmax_length = 512\n</code></pre> <pre><code>cpu\n</code></pre> <pre><code>dataset= load_dataset('imdb', split=['train[45%:55%]', 'test[45%:55%]', 'unsupervised[45%:55%]'])\ndataset = datasets.DatasetDict({\n    \"train\":dataset[0],\n    \"test\":dataset[1],\n    \"unsupervised\":dataset[2]\n\n})\n\nclasses = [\"negative\", \"positive\"]\n\ndataset = dataset.map(\n    lambda x : {\"text_label\": [classes[label] for label in x[\"label\"]]},\n    batched=True,\n    num_proc=1,\n)\ndataset[\"train\"][0]\n</code></pre> <pre><code>Map:   0%|          | 0/2500 [00:00&lt;?, ? examples/s]\n\n\n\nMap:   0%|          | 0/2500 [00:00&lt;?, ? examples/s]\n\n\n\nMap:   0%|          | 0/5000 [00:00&lt;?, ? examples/s]\n\n\n\n\n\n{'text': \"Recap: Full moon. A creature, a huge werewolf, is on the hunt. Not for flesh, not for blood (not that it seem to mind to take a bite on the way though), but for a mate. He is on the hunt for a girl. Not any girl though. The Girl. The girl that is pure (and also a werewolf, although she doesn't know it yet). Three, well check that, two cops (after the first scene) and an old bag lady is all that can stop it, or even knows that the thing killing and eating a lot of folks around full moon is a werewolf. This particular powerful werewolf, Darkwolf, is closing in on the girl. If he gets her, mankind is doomed. Now the cops has to find the girl, convince her not only that there is someone, a werewolf nonetheless, that wants to rape her, and perhaps kill her, but that she is a werewolf herself. And then they got to stop him...&lt;br /&gt;&lt;br /&gt;Comments: This is one for the boys, the teenage boys. A lot of scenes with semi-nude girls more or less important for the plot. Mostly less. Well I guess you need something to fill some time because the plot is (expectedly) thin. And unfortunately there is little besides the girls to help the plot from breaking. One usually turns to two main themes. Nudity. Check. And then special effects. Hmm... Well there are some things that you might call effects. They're not very special though. In fact, to be blunt, they are very bad. The movie seems to be suffering of a lack of funds. They couldn't afford clothes for some of the girls ;), and the effects are cheap. Some of the transformations between werewolf and human form, obviously done by computer, are really bad. You might overlook such things. But the Darkwolf in itself is very crude too, and you never get to see any killings. Just some mutilated corpses afterwards. And there is surprisingly little blood about, in a movie that honestly should be drenched in blood.&lt;br /&gt;&lt;br /&gt;I'm not sure what to say about actors and characters. Most of the times they do well, but unfortunately there are lapses were the characters (or actors) just looses it. A few of these lapses could be connected with the problems mentioned above. Like the poor effects, or the poor budget(?). That could explain why there is precious little shooting, even if the characters are armed like a small army and the target is in plain sight (and not moving). But hey, when you're in real danger, there nothing that will save your life like a good one-liner...&lt;br /&gt;&lt;br /&gt;Unfortunately that can't explain moments when the Cop, Steve, the only one who knows how to maybe deal with the problem, the werewolf that is, runs away, when the only things he can be sure of, is that the werewolf is coming for the girl, who is just beside him now, and that he cannot let it have her. But sure, it let the makers stretch the ending a little more...&lt;br /&gt;&lt;br /&gt;But I wouldn't mind seeing none of the lead actors/actresses get another try in another movie.&lt;br /&gt;&lt;br /&gt;Well. To give a small conclusion: Not a movie that I recommend.&lt;br /&gt;&lt;br /&gt;3/10\",\n 'label': 0,\n 'text_label': 'negative'}\n</code></pre>"},{"location":"notebook/lab-peft-from-scratch/#problems-with-fine-tuning","title":"Problems with Fine-tuning","text":"<p>Fine-tuning large language models like GPT are expensive, present several significant challenges for the average user such as computational expense, storage requirements and operational flexibility when managing multiple fine-tuned models. 1. High computational requirements 2. Expensive storage for checkpoints 3. Slow task switching with multiple fine-tuned models 4. catastrophic forgetting 5. Quality and Representation of Fine-tuning Data</p> <pre><code>import torch \nimport numpy as np\n</code></pre> <pre><code>_ = torch.manual_seed(0)\n# define a rank 2 matrix W of size 10 x 10\nd, k = (10, 10)\nW_r = 2\nW = torch.randn(d, W_r) @ torch.randn(W_r, k)\nprint(W)\nprint(np.linalg.matrix_rank(W))\nprint(W.size())\n\n# apply the singular value decomposition on W\n# W = U x S x V^T\nU, S, V = torch.svd(W)\n# rank r factorization we only keep first r singular value from D and corresponding columns of S and V^T\nU_r = U[:, :W_r]\nS_r = torch.diag(S[:W_r])\nV_r = V[:, :W_r].t()\nprint(S_r)\n# Computing A = U_r x S_r and B = V_r\nA = U_r @ S_r\nB = V_r\nprint(f\"shape of the A is : {A.shape}\")\nprint(f\"shape of the B is : {B.shape}\")\n\n# let's generate the random bias and input \nbias = torch.randn(d)\nx = torch.randn(d)\n\n# computing straight line\n# y = W^Tx + bias\ny = W @ x + bias\n# compute y' = (A * B) x + bias\ny_prime = (A @ B) @ x + bias\n\nprint(f\"original y using W : \\n {y}\")\nprint(f\"computed using BA: \\n {y_prime}\")\n\n# total params elements\nprint(f\"total params of W : {W.nelement()}\")\nprint(f\"total params of A and B: {B.nelement() + A.nelement()}\")\n</code></pre> <pre><code>tensor([[-1.0797,  0.5545,  0.8058, -0.7140, -0.1518,  1.0773,  2.3690,  0.8486,\n         -1.1825, -3.2632],\n        [-0.3303,  0.2283,  0.4145, -0.1924, -0.0215,  0.3276,  0.7926,  0.2233,\n         -0.3422, -0.9614],\n        [-0.5256,  0.9864,  2.4447, -0.0290,  0.2305,  0.5000,  1.9831, -0.0311,\n         -0.3369, -1.1376],\n        [ 0.7900, -1.1336, -2.6746,  0.1988, -0.1982, -0.7634, -2.5763, -0.1696,\n          0.6227,  1.9294],\n        [ 0.1258,  0.1458,  0.5090,  0.1768,  0.1071, -0.1327, -0.0323, -0.2294,\n          0.2079,  0.5128],\n        [ 0.7697,  0.0050,  0.5725,  0.6870,  0.2783, -0.7818, -1.2253, -0.8533,\n          0.9765,  2.5786],\n        [ 1.4157, -0.7814, -1.2121,  0.9120,  0.1760, -1.4108, -3.1692, -1.0791,\n          1.5325,  4.2447],\n        [-0.0119,  0.6050,  1.7245,  0.2584,  0.2528, -0.0086,  0.7198, -0.3620,\n          0.1865,  0.3410],\n        [ 1.0485, -0.6394, -1.0715,  0.6485,  0.1046, -1.0427, -2.4174, -0.7615,\n          1.1147,  3.1054],\n        [ 0.9088,  0.1936,  1.2136,  0.8946,  0.4084, -0.9295, -1.2294, -1.1239,\n          1.2155,  3.1628]])\n2\ntorch.Size([10, 10])\ntensor([[11.3851,  0.0000],\n        [ 0.0000,  4.8439]])\nshape of the A is : torch.Size([10, 2])\nshape of the B is : torch.Size([2, 10])\noriginal y using W : \n tensor([ 7.2684e+00,  2.3162e+00,  7.7151e+00, -1.0446e+01, -8.1639e-03,\n        -3.7270e+00, -1.1146e+01,  2.0207e+00, -9.6258e+00, -4.1163e+00])\ncomputed using BA: \n tensor([ 7.2684e+00,  2.3162e+00,  7.7151e+00, -1.0446e+01, -8.1638e-03,\n        -3.7270e+00, -1.1146e+01,  2.0207e+00, -9.6258e+00, -4.1163e+00])\ntotal params of W : 100\ntotal params of A and B: 40\n</code></pre> <pre><code>plt.plot(x, y)\nplt.show()\n</code></pre> <p></p> <pre><code>plt.plot(x, y_prime)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x7f5099a09810&gt;]\n</code></pre> <p></p> <pre><code>import torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n</code></pre> <ol> <li>Training a NNs to classification of MNIST digits </li> <li>Fine-tuning the NNs on particular digit on which it does't perform well</li> </ol> <pre><code># 1. data transformation pipeline \ntransform = transforms.Compose([\n    transforms.ToTensor(), \n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# 2. loading the training dataset\nmnist_train = datasets.MNIST(\n    root='./dataset',\n    train=True, \n    download=True,\n    transform=transform\n)\n\n# creating the dataset loader for training\ntrain_loader = torch.utils.data.DataLoader(\n    mnist_train,\n    batch_size=10,\n    shuffle=True\n)\n\n# 3. loading the training dataset\nmnist_test = datasets.MNIST(\n    root='./dataset',\n    train=False, \n    download=True,\n    transform=transform\n)\n\n# creating the dataset loader for training\ntest_loader = torch.utils.data.DataLoader(\n    mnist_test,\n    batch_size=10,\n    shuffle=True\n)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\nprint(device)\n</code></pre> <pre><code>cpu\n</code></pre> <pre><code># NNs model for mnist digit classification\nclass NNClassifier(nn.Module):\n    def __init__(self, hiddend_size1=1000, hiddend_size2=2000):\n        super(NNClassifier, self).__init__()\n        self.fc1 = nn.Linear(28*28, hiddend_size1)\n        self.fc2 = nn.Linear(hiddend_size1, hiddend_size2)\n        self.fc3 = nn.Linear(hiddend_size2, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, img):\n        # Returns a new tensor with the same data as the self tensor but of a different shape .\n        x = img.view(-1, 28*28)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = NNClassifier().to(device)\n</code></pre> <pre><code>print(list(model.parameters()))\n</code></pre> <pre><code>[Parameter containing:\ntensor([[-0.0217,  0.0324,  0.0245,  ...,  0.0251, -0.0153,  0.0165],\n        [-0.0317, -0.0023,  0.0119,  ..., -0.0138, -0.0319, -0.0170],\n        [-0.0331,  0.0294, -0.0160,  ..., -0.0044,  0.0223, -0.0177],\n        ...,\n        [-0.0239, -0.0275, -0.0241,  ..., -0.0318, -0.0332,  0.0283],\n        [-0.0185, -0.0276, -0.0083,  ..., -0.0080,  0.0042,  0.0142],\n        [-0.0156, -0.0187, -0.0276,  ..., -0.0276, -0.0033, -0.0178]],\n       requires_grad=True), Parameter containing:\ntensor([-1.0320e-02, -6.5207e-03, -3.0425e-02,  3.1773e-02,  2.3648e-02,\n         2.5797e-02, -5.3310e-03,  2.5184e-02,  2.8565e-02, -3.3112e-02,\n        -1.9557e-02,  7.6884e-03,  2.5767e-02,  1.1022e-02,  1.7873e-02,\n         3.6960e-03,  1.1287e-02, -5.3714e-03,  2.5341e-02,  6.2376e-03,\n        -7.9755e-04, -1.4545e-02, -2.8868e-02,  3.0665e-02, -1.6053e-02,\n         2.4567e-02, -3.2694e-02,  1.0266e-02, -1.8655e-02,  3.0986e-02,\n        -1.3568e-02,  3.4876e-02, -2.9578e-02,  2.7216e-02, -7.4883e-03,\n         3.3605e-02, -1.4872e-03, -2.3862e-02,  1.6431e-02,  3.0644e-03,\n        -2.5285e-02, -3.3244e-02, -3.1994e-02,  1.5422e-02, -1.2993e-02,\n         4.7316e-03, -7.7458e-03, -8.3012e-03,  2.4295e-02,  1.4463e-02,\n         3.0909e-02,  2.0509e-02, -9.2036e-03, -1.5312e-02, -4.0725e-03,\n        -2.4484e-02, -1.4136e-03,  3.1083e-02, -5.4835e-03, -2.4125e-02,\n        -1.4210e-02,  3.0606e-02, -1.4151e-03, -2.4470e-02,  2.0178e-02,\n        -1.6197e-02,  1.7226e-02, -2.5792e-02,  2.6234e-02, -3.2808e-02,\n        -5.0729e-03, -2.6916e-02,  2.6938e-02, -2.9573e-02,  2.2938e-02,\n        -1.9897e-02,  3.3008e-04,  5.4161e-03, -3.3391e-02,  2.9945e-02,\n        -1.7166e-02, -6.3009e-03,  2.0346e-02, -1.4461e-02,  1.8482e-02,\n         6.0949e-03, -7.5448e-03, -3.2886e-02,  2.1630e-02,  7.4188e-03,\n        -3.4950e-02, -8.3634e-03, -1.6844e-02,  2.0370e-02, -1.2094e-02,\n         3.1708e-02, -2.9223e-02,  1.4402e-02,  7.0865e-03, -1.5358e-02,\n         1.1856e-02,  3.1122e-02,  1.5203e-02, -1.8215e-03, -1.2824e-02,\n         1.0726e-02,  1.1917e-02, -1.0603e-02, -2.5398e-02,  1.4085e-02,\n         3.3672e-02,  2.1310e-02, -2.9447e-02,  9.3373e-03, -1.7143e-02,\n         1.7928e-02,  2.4010e-02,  3.1620e-03, -3.4073e-02, -1.7843e-04,\n         2.1446e-02, -5.7481e-03, -5.3269e-03,  2.2068e-02, -1.4882e-02,\n         2.1116e-02,  1.4975e-02,  3.0249e-02, -1.8176e-02, -2.0800e-02,\n        -3.2717e-02, -1.3635e-02, -2.2269e-03, -2.3452e-02,  6.2030e-03,\n        -3.7176e-03, -3.2942e-02,  1.6464e-02,  2.1308e-03,  9.1494e-03,\n         3.0661e-02, -1.0450e-02,  3.5434e-02, -1.0500e-02, -1.3349e-02,\n         6.4561e-03, -2.2488e-02, -1.9313e-02, -2.1507e-02,  2.9707e-02,\n        -1.0514e-03, -2.2687e-02,  1.6495e-02, -7.2441e-03,  9.0183e-03,\n         3.5323e-02,  2.3491e-02,  3.4227e-02, -3.5294e-02,  3.1196e-02,\n        -3.0435e-02, -1.8756e-03,  3.1710e-02, -2.5314e-02,  1.9563e-02,\n        -5.1431e-03, -2.2396e-02,  1.6726e-02,  2.7843e-02,  1.9943e-03,\n         9.1732e-03, -2.1799e-03,  1.3088e-02,  1.4959e-02, -1.8214e-02,\n        -2.5891e-02,  1.7134e-02, -1.3138e-02, -3.4261e-02, -2.4443e-03,\n         1.6028e-02,  9.3183e-03,  2.9106e-03, -1.4481e-02,  3.0286e-02,\n         8.1368e-03, -2.3945e-02,  1.9110e-02,  1.2722e-02, -2.6075e-03,\n        -2.9834e-02,  1.8304e-02, -3.1546e-02,  2.7695e-02,  3.7760e-03,\n         1.4670e-02, -6.9188e-03,  1.0402e-02, -9.1942e-03,  3.4181e-02,\n         2.2536e-02, -3.1036e-02,  3.3324e-02,  8.9956e-03, -3.4604e-02,\n         1.9387e-02,  1.8981e-02, -8.9051e-03,  3.0992e-02, -1.2215e-02,\n         2.0103e-02,  2.2554e-02, -3.3443e-02,  3.0147e-02,  1.9108e-02,\n        -2.4510e-02, -1.2634e-02, -2.1231e-02,  3.4975e-02,  2.0375e-02,\n         7.2947e-03,  4.3169e-03, -1.2881e-02, -1.3409e-02, -2.1335e-02,\n        -2.3977e-02,  2.3418e-02,  6.6731e-03,  3.4438e-02, -2.0304e-02,\n         3.2077e-03, -3.0646e-02, -2.4091e-02,  1.9362e-02, -3.6200e-03,\n        -2.1357e-02,  7.0748e-03,  2.3027e-02, -1.3675e-04,  1.2392e-02,\n        -3.3867e-02, -2.4129e-02, -2.3819e-02,  2.6258e-02,  7.8028e-03,\n        -1.3049e-02,  3.2535e-02,  1.7173e-02,  3.3982e-02,  1.2429e-02,\n         3.4691e-03, -3.6540e-03,  2.7349e-02,  1.3865e-02, -3.0865e-02,\n         2.5375e-04,  1.4413e-02,  1.8267e-02,  1.9683e-02,  2.0639e-02,\n         3.5192e-02,  3.5667e-03,  3.1047e-02, -2.9158e-03,  1.9225e-02,\n        -8.5528e-03,  3.1992e-02, -1.0818e-02, -2.1563e-02,  8.3119e-03,\n        -1.7559e-02,  2.6640e-02, -5.2229e-03, -2.5551e-02,  4.8802e-03,\n         1.4288e-02, -6.5380e-03,  2.4955e-02, -5.4183e-03, -1.4058e-02,\n         2.6932e-03,  3.3208e-02, -6.2700e-03, -1.5788e-02, -1.7684e-02,\n         2.9466e-02,  2.7513e-02,  2.9602e-02, -3.3953e-02,  3.5185e-02,\n        -2.7161e-02,  2.4324e-02, -3.3283e-02,  2.2554e-02, -5.0453e-03,\n         2.2500e-02,  2.1988e-02, -2.4596e-02, -1.3986e-02,  2.7671e-02,\n        -3.4754e-03, -1.2286e-02,  1.4984e-02, -4.4056e-03, -8.6189e-03,\n         2.7208e-02, -2.9294e-02,  1.6323e-02, -3.1894e-02, -3.9037e-03,\n        -3.3732e-02,  1.0061e-02,  2.4544e-02, -6.7358e-03,  2.8664e-02,\n         3.3990e-02,  2.9531e-02,  1.7360e-02, -3.0523e-03, -1.9472e-03,\n         3.5627e-02,  3.3541e-02, -3.4791e-02, -1.1263e-02, -1.1480e-02,\n        -2.3141e-02, -3.3590e-02,  1.1438e-02,  2.7258e-02,  2.4727e-02,\n        -3.0529e-02, -1.0583e-02, -2.0665e-02,  3.1055e-02, -5.0806e-03,\n        -2.0507e-02,  2.5286e-02, -1.3779e-03, -2.2157e-02,  6.1207e-03,\n         3.2355e-02, -1.8250e-02, -3.1861e-02, -9.3196e-03,  2.1641e-02,\n        -3.9089e-03,  3.4388e-03, -2.2020e-02, -4.4128e-03,  2.3871e-03,\n         3.2001e-02, -2.5075e-03,  2.8722e-02, -4.2344e-03, -2.9219e-02,\n         2.0086e-03,  1.5973e-02, -3.5049e-03,  2.1306e-02, -1.1166e-02,\n         2.0079e-02, -6.7300e-03, -2.6517e-04, -1.1985e-02, -1.7145e-02,\n         1.9042e-02,  2.2402e-02,  1.1510e-02, -2.1869e-02, -2.6135e-02,\n        -2.9119e-03,  1.1450e-02,  1.6710e-02,  4.8621e-03,  1.9973e-02,\n        -1.0327e-02, -2.5088e-02, -2.5622e-03, -3.3391e-02,  1.4364e-02,\n        -8.9138e-03,  1.4032e-02,  2.5126e-02, -1.6650e-02, -1.9333e-02,\n         2.2969e-03, -2.5163e-02, -2.5816e-02,  9.7195e-03,  1.2432e-02,\n        -2.8958e-02,  6.3350e-03,  3.3329e-02,  2.0824e-02, -1.2263e-02,\n        -1.0363e-02, -9.2727e-03, -2.2585e-02, -2.0626e-02,  1.9297e-02,\n         7.4085e-03,  1.7134e-03, -1.3231e-02,  2.3401e-02,  1.3387e-02,\n         3.1168e-02,  1.1417e-02,  1.5655e-02,  3.2483e-02, -2.0565e-02,\n        -1.7921e-02,  8.7170e-03, -3.4215e-02, -2.3987e-02,  2.6542e-02,\n        -3.4977e-02,  1.9865e-02, -8.3847e-03, -2.9331e-02,  1.5026e-02,\n        -3.4773e-02,  2.6966e-02,  2.7601e-02, -1.2405e-02,  1.7106e-02,\n        -3.0025e-03, -1.5765e-02, -2.2292e-02, -2.9382e-02,  1.4102e-02,\n        -5.2560e-03,  2.1957e-03, -2.1462e-02, -1.4539e-02, -1.0104e-02,\n        -2.3779e-02,  3.4817e-02,  3.5219e-02, -3.8881e-03, -6.7443e-03,\n        -2.3840e-02,  2.8530e-02,  2.9175e-02,  1.3148e-02, -1.1325e-02,\n        -1.1776e-02, -1.7442e-02, -1.2934e-02,  7.2415e-03, -1.9589e-02,\n        -1.4543e-02,  9.6600e-03,  1.3898e-02, -2.8426e-02,  4.4072e-03,\n         9.4081e-03,  1.6309e-03,  3.0097e-02, -1.1128e-02,  3.6528e-03,\n        -9.2725e-03,  1.3974e-02, -2.6080e-02,  2.0842e-02, -2.1798e-04,\n         1.6775e-02, -2.4995e-02, -2.8316e-02, -1.4493e-02, -2.8528e-02,\n         2.5058e-02, -8.3217e-03, -3.3703e-02, -4.7767e-03, -2.3862e-02,\n        -2.0058e-02,  3.2306e-02,  2.9167e-02, -1.4795e-02,  5.3299e-03,\n        -2.5088e-02,  1.2088e-02, -2.5961e-02, -7.5826e-03, -3.0156e-02,\n        -1.8749e-02, -2.6915e-02, -2.9662e-03, -8.0834e-03,  2.6074e-02,\n         2.1337e-02,  3.3836e-02, -1.4253e-02,  2.5713e-02,  1.0277e-02,\n         2.6883e-02,  2.5550e-02,  7.3263e-04, -2.1505e-02, -2.9476e-02,\n         1.7300e-02, -9.2658e-04, -2.6885e-02,  3.1527e-02, -3.1505e-02,\n         1.6972e-03,  3.9439e-03, -1.3458e-02,  2.0468e-02, -1.3039e-02,\n        -2.6961e-02,  2.1360e-02, -1.9148e-02,  9.4469e-03,  3.8634e-03,\n         1.5208e-02, -3.4737e-02,  2.8281e-02,  1.1064e-02,  8.2391e-03,\n         3.2464e-02, -1.1658e-02,  1.3489e-02, -1.7283e-02,  2.2119e-02,\n        -3.4424e-02,  2.5421e-02, -1.2789e-02,  2.9809e-02,  1.0789e-02,\n         2.3867e-02, -3.5295e-02,  2.3215e-02,  1.9149e-02, -2.4315e-02,\n        -2.5146e-02, -8.6107e-04, -5.0835e-03, -1.8575e-02, -2.6215e-02,\n        -2.7810e-02,  2.5646e-02,  3.0134e-02, -3.0501e-02, -2.9124e-02,\n        -8.9983e-03,  3.3040e-02,  3.3356e-02, -1.9703e-02,  1.0565e-02,\n        -2.8867e-02, -1.3825e-02, -3.0619e-02,  2.5332e-02,  2.1241e-03,\n        -3.4285e-02,  9.5266e-03, -9.6307e-03, -3.0483e-02, -8.1293e-04,\n        -1.9215e-02, -2.6393e-02,  1.0839e-02,  1.7522e-02,  1.2732e-02,\n         2.4161e-03,  3.2747e-03, -2.7306e-02, -1.6981e-02, -2.5289e-02,\n         1.4391e-02,  1.5814e-02, -3.2952e-02,  8.5922e-03, -1.8989e-02,\n         2.6121e-02,  2.6907e-02, -2.3459e-02, -1.6141e-02, -3.3371e-02,\n        -2.6357e-03,  2.4567e-02, -7.7612e-03, -2.1023e-02,  2.5129e-02,\n         2.1122e-02, -8.7298e-03, -1.4417e-02, -3.4422e-05, -3.5433e-03,\n        -2.5052e-02, -6.2602e-03,  1.9767e-02,  2.8703e-02,  2.2393e-02,\n        -2.6030e-02,  2.7101e-02,  2.2300e-02, -8.3532e-03, -2.9488e-02,\n         1.3485e-02,  4.4590e-03,  2.7863e-02, -4.9552e-03, -1.2408e-02,\n         2.1340e-02,  3.3019e-02, -2.0870e-02, -1.0793e-02, -1.7556e-02,\n        -1.8153e-02, -2.9978e-02, -1.5054e-04, -7.9508e-03, -2.0383e-02,\n         1.8043e-03, -2.5907e-02,  2.1147e-03, -5.1263e-03, -8.3881e-03,\n        -9.3727e-04, -2.5783e-02, -6.2917e-03, -2.2286e-02,  7.0261e-03,\n        -2.6878e-02, -3.3122e-02,  2.7937e-02,  1.0864e-02,  2.6594e-02,\n        -1.1931e-02,  5.8170e-03,  2.6352e-02,  1.2630e-02, -9.7866e-03,\n        -1.6081e-04, -1.4795e-03,  2.5824e-02,  2.9091e-02,  1.1263e-02,\n         1.0342e-02,  8.4446e-03,  3.1705e-02,  1.8338e-02, -1.8277e-02,\n         8.5916e-04, -2.4103e-02,  1.7859e-02, -9.5928e-03,  1.3742e-02,\n        -8.9115e-03,  1.4277e-02,  3.5388e-02, -1.7607e-02, -1.4693e-02,\n        -9.8005e-03,  4.3073e-05, -3.3865e-02,  1.0280e-02,  1.5807e-02,\n         1.5070e-02, -1.6566e-02,  1.7564e-02, -2.7704e-02, -2.2590e-02,\n         8.8178e-03, -1.7540e-02,  2.6702e-02, -3.1916e-03,  1.7622e-02,\n        -7.6404e-03,  2.8830e-02, -1.3591e-02,  2.4009e-02,  1.5386e-02,\n         6.9780e-03,  1.8959e-02,  8.2545e-03, -6.6080e-03, -3.2939e-02,\n         2.8128e-02, -1.7068e-02,  5.5168e-04, -5.2053e-03, -2.9445e-02,\n         3.1236e-02,  1.9677e-02, -4.3910e-03, -3.2076e-02,  1.0516e-02,\n        -3.1377e-02,  1.9781e-02,  3.1435e-02,  2.1929e-02,  1.3153e-02,\n        -2.6972e-02,  2.1385e-02,  3.4706e-02,  1.4306e-02, -5.6461e-03,\n         2.7818e-02,  5.8553e-03,  1.9443e-02,  2.9906e-02, -2.3895e-02,\n        -4.4953e-03,  3.4025e-02, -2.7327e-02,  7.5911e-03, -1.0696e-02,\n        -1.5784e-02, -3.3610e-02,  2.1257e-02, -8.0741e-03, -2.5527e-02,\n        -2.9305e-02, -6.2372e-03,  6.2000e-03, -6.2799e-03,  2.2755e-02,\n        -3.3834e-03, -8.0613e-03, -1.8107e-02,  4.3210e-03, -6.5641e-03,\n        -2.9937e-02, -1.0537e-02, -1.7394e-02,  3.1030e-02, -3.5578e-02,\n        -7.2662e-03, -4.8714e-03, -2.2475e-02, -5.1476e-03, -1.6219e-02,\n         1.8819e-03, -2.7253e-02,  2.8513e-02, -2.1133e-02,  2.4286e-02,\n        -1.1441e-02,  1.7717e-02,  3.2361e-03,  2.7704e-02, -2.1761e-02,\n        -2.2392e-02,  3.1089e-02,  2.9018e-02,  3.0272e-02, -6.8205e-03,\n        -2.5453e-02,  2.3631e-02, -2.0008e-02, -1.3758e-02, -3.0179e-02,\n         9.0209e-03,  3.5342e-02,  1.8764e-02, -2.9572e-03,  1.4037e-02,\n        -2.5208e-02, -7.9729e-03, -3.3988e-02, -2.0911e-02,  2.1514e-02,\n         3.1382e-02,  6.7151e-03, -9.3605e-03,  2.7419e-02,  6.9611e-03,\n        -1.8635e-02,  3.1656e-02, -1.4793e-02,  2.3778e-03, -1.7655e-02,\n        -3.4396e-02, -1.3742e-02,  3.0899e-02,  8.0773e-03,  2.6538e-02,\n        -1.9777e-02, -9.0255e-03,  1.1360e-02,  1.3888e-02,  1.7534e-02,\n         3.1802e-02, -3.1403e-02,  2.4403e-02, -9.1145e-03,  1.8790e-02,\n        -1.3794e-02,  6.3013e-03, -9.9022e-04,  5.3809e-03, -4.6361e-04,\n         1.2676e-02, -2.3639e-02,  3.1625e-02, -2.5981e-02, -3.2198e-02,\n         1.5652e-02,  1.8372e-02, -2.6063e-02,  1.2784e-02, -8.7744e-03,\n        -3.2363e-02, -1.1799e-02, -2.8773e-03,  3.6047e-03,  3.3369e-02,\n         1.8367e-02, -4.9440e-03, -2.5065e-02, -2.4709e-02, -1.0674e-02,\n        -2.2570e-02, -1.0224e-02, -1.2301e-02, -1.2572e-02,  6.1015e-03,\n         2.5827e-02, -2.7389e-02,  8.3895e-03, -1.4418e-02,  2.3136e-02,\n         1.8191e-03, -9.2161e-03, -1.1298e-02, -3.5630e-02, -9.3792e-03,\n        -1.6237e-02, -1.5355e-02, -3.5711e-02,  4.5125e-03, -4.9696e-03,\n         2.7394e-03, -7.9843e-03,  1.4771e-02, -2.9733e-02, -1.8549e-03,\n         1.6829e-02,  3.3979e-02,  9.2009e-03, -1.2639e-02, -3.4870e-02,\n        -2.8328e-02,  2.7494e-03, -3.5614e-02, -3.2945e-02,  3.1098e-03,\n        -1.7553e-03, -2.7198e-02, -4.6992e-04,  2.2827e-02,  2.8319e-02,\n        -5.5863e-03, -2.9454e-02, -1.0707e-02, -2.1160e-02,  6.5313e-03,\n        -1.6470e-02,  1.3873e-02,  2.9606e-02, -1.9141e-02, -1.2932e-02,\n        -1.1128e-02, -2.7259e-02, -8.5404e-03,  1.4063e-03, -1.5799e-02,\n        -5.2531e-04,  6.8968e-03, -3.2139e-02, -2.1133e-02,  5.9622e-04,\n        -2.4588e-02,  2.4882e-02, -2.0747e-02,  3.2070e-02, -1.7916e-02,\n        -3.1254e-02,  7.4483e-03, -2.6131e-02,  2.1663e-02, -1.9201e-02,\n         1.8842e-02, -7.4075e-03,  7.4798e-03, -2.9741e-02,  8.1354e-03,\n        -2.7016e-02,  2.3997e-02,  2.4825e-02,  3.3629e-02,  2.5536e-02,\n        -1.3840e-02, -2.8280e-02, -2.0921e-02,  9.3194e-04,  7.7197e-03,\n        -1.5052e-02, -6.9257e-03,  3.1434e-02,  1.7373e-04,  1.4073e-02,\n         2.0773e-02,  1.4547e-02, -8.5811e-03, -2.3977e-02, -3.3969e-02,\n         3.0125e-02, -6.3217e-03, -2.1554e-02,  2.5343e-02, -2.9926e-02,\n        -2.3997e-03,  3.9591e-03, -5.7528e-03,  3.4311e-02,  1.6726e-02,\n         1.7948e-02, -3.3646e-02, -2.2943e-02, -1.5972e-02, -3.4631e-02,\n         9.5554e-03, -1.9555e-02, -2.2307e-02, -7.5593e-03, -5.3825e-03,\n        -1.4440e-02, -1.5396e-02,  2.6183e-02, -2.9610e-02,  1.3939e-02,\n        -1.1712e-02, -1.1865e-02,  3.0449e-02,  2.1924e-02,  2.5383e-03,\n        -1.2323e-02,  1.2927e-02, -1.1346e-02,  9.4067e-03,  7.2429e-03,\n        -1.9180e-02, -3.7484e-03, -2.7377e-02, -1.2778e-02, -2.9293e-02,\n         2.1381e-02,  2.7661e-02,  3.4644e-02, -1.4897e-02, -1.9537e-02,\n         6.6081e-03,  1.4900e-02, -7.8494e-03,  1.8571e-02,  1.7224e-02,\n        -1.4418e-02, -3.3899e-02, -2.1095e-02, -2.1899e-02, -1.3322e-02,\n        -2.4567e-02, -1.7855e-02, -1.4534e-02,  1.5108e-02, -8.2170e-03,\n        -2.1554e-02,  2.1186e-02,  8.7807e-03,  3.2396e-02, -5.4814e-03,\n         1.1285e-02, -2.0407e-02,  1.3838e-02,  1.7412e-02,  6.5471e-04,\n         2.2826e-02,  2.5913e-02, -1.3132e-02,  2.4417e-02,  3.7038e-03,\n        -3.1471e-02,  3.5355e-02,  1.0973e-03,  5.5835e-03,  9.1416e-03,\n         1.3847e-02,  2.7203e-02,  2.6725e-02,  2.8856e-02,  1.4015e-03,\n         2.8019e-02, -2.5718e-02, -6.4135e-03,  1.3450e-02,  2.1297e-02],\n       requires_grad=True), Parameter containing:\ntensor([[ 2.7064e-02, -1.3890e-02,  1.2426e-02,  ...,  1.6834e-02,\n         -2.2114e-02, -1.2481e-03],\n        [ 8.6761e-04, -2.9554e-02,  1.0315e-02,  ..., -1.0449e-03,\n          2.7714e-02, -1.7915e-02],\n        [-4.0925e-03,  1.9115e-02, -2.9762e-02,  ...,  1.9335e-02,\n          2.7546e-02,  1.9274e-02],\n        ...,\n        [-1.8041e-02, -1.8621e-02,  1.5718e-02,  ..., -1.9459e-02,\n         -1.5317e-02, -6.8762e-03],\n        [ 2.0010e-03,  5.0789e-03, -6.4813e-03,  ..., -2.7187e-05,\n          1.8645e-02,  3.6312e-03],\n        [-1.2214e-02, -7.0958e-03,  2.7411e-02,  ...,  1.2113e-03,\n         -3.0699e-02,  1.4975e-02]], requires_grad=True), Parameter containing:\ntensor([ 0.0277, -0.0068, -0.0157,  ..., -0.0305,  0.0262, -0.0049],\n       requires_grad=True), Parameter containing:\ntensor([[ 0.0028, -0.0114,  0.0146,  ..., -0.0143,  0.0011,  0.0098],\n        [-0.0015, -0.0114, -0.0172,  ..., -0.0144, -0.0206,  0.0201],\n        [ 0.0171, -0.0212,  0.0021,  ..., -0.0209, -0.0052, -0.0055],\n        ...,\n        [-0.0084, -0.0177,  0.0097,  ..., -0.0002, -0.0125,  0.0055],\n        [ 0.0217,  0.0042,  0.0052,  ..., -0.0189, -0.0043, -0.0173],\n        [ 0.0069,  0.0023,  0.0138,  ...,  0.0218,  0.0053,  0.0042]],\n       requires_grad=True), Parameter containing:\ntensor([-0.0203, -0.0220,  0.0044, -0.0117, -0.0069,  0.0038,  0.0209, -0.0037,\n        -0.0203,  0.0065], requires_grad=True)]\n</code></pre> <pre><code>def training(train_loader, model, epochs, iter_limit=None):\n    # loss function and optimizer\n    loss_fun = nn.CrossEntropyLoss()\n    optimizer= torch.optim.Adam(model.parameters(), lr=0.001)\n\n    iter_total=0\n    for epoch in range(epochs):\n        model.train()\n\n        loss_sum = 0\n        iter_num = 0\n\n        data_iter = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n        if iter_limit is not None:\n            data_iter.total = iter_limit\n        for data in data_iter:\n            iter_num += 1\n            iter_total +=1\n\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n\n            output = model(x.view(-1, 28*28))\n            loss  = loss_fun(output, y)\n\n\n            loss_sum += loss.item()\n\n            avg_loss = loss_sum / iter_num\n\n            data_iter.set_postfix(loss=avg_loss)\n            loss.backward()\n            optimizer.step()\n\n            if iter_limit is not None and iter_total &gt;= iter_limit:\n                return avg_loss\n    return avg_loss\n</code></pre> <pre><code>training(train_loader, model, epochs=1)\n</code></pre> <pre><code>Epoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [01:43&lt;00:00, 58.11it/s, loss=0.24]\n\n\n\n\n\n0.24042963718108756\n</code></pre> <pre><code># make a copy of the original model's weight, we use it to prive that LoRA fine-tuning does't alter the original weights\norg_weights = {}\nfor name, param in model.named_parameters():\n    org_weights[name] = param.clone().detach()\n</code></pre> <pre><code># the performance of  NNClassifier model\n# It poorly performed on digit 2\n# so we will fine-tune the model for digit 2\n\ndef test():\n    correct = 0\n    total = 0\n    wrong_counts = [0 for i in range(10)]\n\n    with torch.no_grad():\n        for data in tqdm(test_loader, desc='Testing'):\n            x, y = data \n            x  = x.to(device)\n            y = y.to(device)\n\n            output = model(x.view(-1, 784))\n            for idx, i in enumerate(output):\n                if torch.argmax(i) == y[idx]:\n                   correct +=1\n                else:\n                    wrong_counts[y[idx]] +=1\n                total += 1\n    print(f\"Accuracy:{round(correct/total, 3)}\")\n    for i in range(len(wrong_counts)):\n        print(f\"Wrong count for digit {i}: {wrong_counts[i]}\")\n\ntest()\n</code></pre> <pre><code>Testing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 1041.01it/s]\n\nAccuracy:0.959\nWrong count for digit 0: 10\nWrong count for digit 1: 11\nWrong count for digit 2: 103\nWrong count for digit 3: 37\nWrong count for digit 4: 57\nWrong count for digit 5: 38\nWrong count for digit 6: 19\nWrong count for digit 7: 70\nWrong count for digit 8: 37\nWrong count for digit 9: 33\n</code></pre> <pre><code># How many parameters are in the our model(NNClassifier) ?\n# 1. print the size of weight matrix of the NNs\n# 2. save the count of total number of parameters\ntotal_params_org = 0\nfor index , layer in enumerate([model.fc1, model.fc2, model.fc3]):\n    total_params_org += layer.weight.nelement() + layer.bias.nelement()\n\n    print(f'Layer {index + 1} - W : {layer.weight.shape} + B: {layer.bias.shape}')\n\nprint(f'Total number of parameters: {total_params_org}')\n</code></pre> <pre><code>Layer 1 - W : torch.Size([1000, 784]) + B: torch.Size([1000])\nLayer 2 - W : torch.Size([2000, 1000]) + B: torch.Size([2000])\nLayer 3 - W : torch.Size([10, 2000]) + B: torch.Size([10])\nTotal number of parameters: 2807010\n</code></pre> <pre><code>class LoRaParameterization(nn.Module):\n   def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n      super().__init__()\n\n      # random gaussian initialization for a and zero for B\n      self.A = nn.Parameter(torch.zeros(rank, features_out)).to(device)\n      nn.init.normal_(self.A, mean=0, std=1)\n      self.B = nn.Parameter(torch.zeros(features_in, rank)).to(device)\n\n      # scale \u2206Wx by \u03b1/r , where \u03b1 is a constant in r.\n      # When optimizing with adam, tuning \u03b1 is roughly the same as tuning the learning rate if we scale the initialization appropriately.\n      #   as a result, we simply set \u03b1 to the first r we try and do not tune it.\n      #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n      self.scale = alpha / rank\n      self.enabled = True\n   def forward(self, org_weights):\n      if self.enabled:\n         return org_weights + torch.matmul(self.B, self.A).view(org_weights.shape)\n      else:\n         return org_weights\n</code></pre> <pre><code># addition of the parameters to our NNs\nimport torch.nn.utils.parametrize as parametrize\n\ndef LinearLayerParameterization(layer, device, rank=1, lora_alpha=1):\n    # we only add parameterization to the weight matrix and ignore the bias\n    # We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n    # We leave the emphirical investigation of [...] and biases to a future work\n\n    features_in, features_out = layer.weight.shape\n\n    return LoRAParameterization(\n        features_in,\n        features_out,\n        rank=rank,\n        alpha=lora_alpha,\n        device=device\n    )\n\nparametrize.register_parametrization(\n    model.fc1, \n    \"weight\",\n    LinearLayerParameterization(model.fc1, device)\n)\n\nparametrize.register_parametrization(\n    model.fc2, \n    \"weight\",\n    LinearLayerParameterization(model.fc2, device)\n)\n\nparametrize.register_parametrization(\n    model.fc3, \n    \"weight\",\n    LinearLayerParameterization(model.fc3, device)\n)\n\ndef EnableDesableLoRA(enable=True):\n    for layer in [model.fc1, model.fc2, model.fc3]:\n        layer.parameterization[\"weight\"][0].enabled = enabled\n</code></pre> <pre><code>total_param_lora = 0\ntotal_param_non_lora = 0\nfor index, layer in enumerate([model.fc1, model.fc2, model.fc3]):\n    total_param_lora += layer.parametrizations[\"weight\"][0].A.nelement() + layer.parametrizations[\"weight\"][0].B.nelement()\n    total_param_non_lora += layer.weight.nelement() + layer.weight.nelement()\n\n    print(\n       f'Layer {index+1}: W: {layer.weight.shape} + Bias: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].B.shape}'\n    )\n\n\n# The non-LoRA param count must match the original network\n\nprint(f'Total number of param (original): {total_param_non_lora:,}')\nprint(f'Total number of param (original + LoRA): {total_param_lora + total_param_non_lora:,}')\nprint(f'Param introduced by LoRA: {total_param_lora:,}')\nparam_incremment = (total_param_lora / total_param_non_lora) * 100\nprint(f'Param incremment: {param_incremment:.3f}%')\n</code></pre> <pre><code>Layer 1: W: torch.Size([1000, 784]) + Bias: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\nLayer 2: W: torch.Size([2000, 1000]) + Bias: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\nLayer 3: W: torch.Size([10, 2000]) + Bias: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\nTotal number of param (original): 5,608,000\nTotal number of param (original + LoRA): 5,614,794\nParam introduced by LoRA: 6,794\nParam incremment: 0.121%\n</code></pre> <pre><code># 1. Freeze all params of the original model\nfor name, param in model.named_parameters():\n    if 'lora' not in name:\n        print(f\"Freezing Original params of models :  {name}\")\n        param.requires_grad=False\n\n\n\n# 2. Finetuning the models params that introduced by LoRA on training digit 2\n# let's load the mnist data set and keeping only the digit 2\ntrain = datasets.MNIST(\n    './dataset',\n    train=True,\n    download=True,\n    transform=transform,\n    )\nexclud_idx = train.targets = 2\ntrain.data = train.data[exclud_idx]\ntrain.targets = train.targets[exclud_idx]\n\n# creating the dataloader for the training\ntrain_loader = torch.utils.data.DataLoader(\n    train,\n    batch_size=10,\n    suffle=True)\n\n# Train the network with LoRA only on the digit 2 and only for 100 batches (hoping that it would improve the performance on the digit 9)\ntrain(train_loader, model, epochs=1, total_iterations_limit=100)\n</code></pre> <pre><code># Verify that the fine-tuning didn't alter the original weights, but only the ones introduced by LoRA.\n# Check that the frozen parameters are still unchanged by the finetuning\nassert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\nassert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\nassert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n\nenable_disable_lora(enabled=True)\n# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n# The original weights have been moved to net.linear1.parametrizations.weight.original\n# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\nassert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n\nenable_disable_lora(enabled=False)\n# If we disable LoRA, the linear1.weight is the original one\nassert torch.equal(net.linear1.weight, original_weights['linear1.weight'])\n</code></pre> <pre><code>#Test the network with LoRA enabled (the digit 9 should be classified better)\n# Test with LoRA enabled\nenable_disable_lora(enabled=True)\ntest()\n# Test the network with LoRA disabled (the accuracy and errors counts must be the same as the original network)\n# Test with LoRA disabled\nenable_disable_lora(enabled=False)\ntest()\n</code></pre>"},{"location":"notebook/lab-peft-from-scratch/#reference","title":"Reference","text":"<ol> <li>https://huggingface.co/EleutherAI/gpt-neo-125m</li> <li>https://huggingface.co/google-t5/t5-small</li> <li>https://huggingface.co/datasets/stanfordnlp/imdb</li> </ol>"},{"location":"notebook/lab-peft-from-scratch/#tutorials","title":"Tutorials","text":"<ol> <li>Problem : https://colab.research.google.com/drive/1TyF-FmHN1Yd72qdaX-Z_a6kpcvWmH1FP?usp=sharing#scrollTo=_8C1P8OnezTP</li> <li>Solutions : https://colab.research.google.com/drive/1t0FqAqS2m3eGHsSKlQnlidgrW4IcHjCw?usp=sharing#scrollTo=X7Eb4LrVzUM0</li> <li> <p>Summer School : https://colab.research.google.com/drive/1OkqcpLVbze_obiomPakArA5kabWbn3lO#scrollTo=kKGxLfu0wS-U</p> </li> <li> <p>HandsOn Tutorial Notebook by Prof. Ashutosh Modi: https://tinyurl.com/PEFT-HandsOn-Sol</p> </li> <li>Huggingface PEFT Library: https://github.com/huggingface/peft/tree/main</li> <li>LoRA Paper: https://arxiv.org/pdf/2106.09685</li> <li>LoRA Explanation by Umar Jamil - https://www.youtube.com/watch?v=PXWYUTMt-AU</li> <li>QLoRA Paper: https://arxiv.org/pdf/2305.14314</li> <li>OLoRA Paper Explained: https://www.youtube.com/watch?v=6l8GZDPbFn8</li> <li>PEFT Hugging Face : https://huggingface.co/blog/samuellimabraz/peft-methods</li> </ol>"},{"location":"notebook/lab-prompting/","title":"Lab-111 : LLM Prompting","text":"<ol> <li>What is Large Language Models(LLMs) ?</li> </ol> <p>LLMs like Falcon, LLaMA etc are pretrained transformers models initially trained to predict the next token given some input text. LLM model size having several hundred billions of parameters and have been trained on trillions of tokens for an extended period of time on internet data and fraction of universe knowledge dataset. We can use these models to solve multiple NLP tasks out of the box by instructing the models with natural language prompts.</p> <p>Our aim to design prompt to ensure the optimal output. Prompt Engineering - \"An iterative process that requires a fair amount of experimentation\". We know that natural languages are more flexible and expressive than the programming languages therefore they can introduce some ambiguity. The prompts in natural languages are quite sensitive to changes because minore change can leads to quite different outputs. There is no best algorithm for creating prompt but researcher have figure out best practices for creating optimal results that are more consistent with better LLM prompts.</p> <p>Prompting</p> <p>Modern LLMs : Decoder-only transformers - Llama2, Falcon, GPT2</p> <p>Encoder-decoder transformers - Flan-T5, BART</p> <p>Encoder-decoder-style models are used in generative task where output approx totally dependent on input like translation and summarization etc.</p> <p>Decoder-only transformer model are usefull for all others types of generative tasks.</p> <p>We will use pipeline to generate text with LLM and each LLM might have different pipeline.</p> <p>Most LLM checkpoints available in two versions : base and instruct on huggingface-hub like  <code>tiiuae/falcon-7b</code> and <code>tiiuae/falcon-7b-instruct</code>.</p> <p>What are Base Models ? Base models are pretrained llms which are excellent at completing the text when given an initial prompt, however, they are not ideal for NLP tasks where they need to follow instructions, or for conversational use.</p> <p>What are Instruct Models ? Instruct Models are also pretrained model and when we finetune the base model on instructions and conversational data these result in checkpoints which makes them better choice for many NLP tasks according to instructions and conversational data.</p> <pre><code>!pip install -q transformers accelerate\n</code></pre> <pre><code>\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\n</code></pre> <pre><code># depenencies\nfrom transformers import pipeline, AutoTokenizer\nimport torch\n</code></pre> <pre><code># Text generation task with decoder-only models i.e running inferences from gpt-2 model\ntorch.manual_seed(0)\ngen = pipeline('text-generation',\n               model = 'openai-community/gpt2')\n</code></pre> <pre><code>/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\nconfig.json:   0%|          | 0.00/665 [00:00&lt;?, ?B/s]\n\n\n\nmodel.safetensors:   0%|          | 0.00/548M [00:00&lt;?, ?B/s]\n\n\n\ngeneration_config.json:   0%|          | 0.00/124 [00:00&lt;?, ?B/s]\n\n\n\ntokenizer_config.json:   0%|          | 0.00/26.0 [00:00&lt;?, ?B/s]\n\n\n\nvocab.json:   0%|          | 0.00/1.04M [00:00&lt;?, ?B/s]\n\n\n\nmerges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]\n\n\n\ntokenizer.json:   0%|          | 0.00/1.36M [00:00&lt;?, ?B/s]\n\n\nDevice set to use cuda:0\n</code></pre> <pre><code>prompt = \"Hello, I'm a large language model\"\ngen(prompt, max_length=40)\n</code></pre> <pre><code>Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n\n\n\n[{'generated_text': \"Hello, I'm a large language model. Not because I love writing. I love to write software.\\n\\nBut if you look carefully at the code I wrote in the previous entry, there are\"}]\n</code></pre> <pre><code># text2text-generation with encoder-decoder transformer model, performing inferences with pipeline\ntext2text_gen = pipeline('text2text-generation',\n                         model = 'google/flan-t5-base')\n</code></pre> <pre><code>config.json:   0%|          | 0.00/1.40k [00:00&lt;?, ?B/s]\n\n\n\nmodel.safetensors:   0%|          | 0.00/990M [00:00&lt;?, ?B/s]\n\n\n\ngeneration_config.json:   0%|          | 0.00/147 [00:00&lt;?, ?B/s]\n\n\n\ntokenizer_config.json:   0%|          | 0.00/2.54k [00:00&lt;?, ?B/s]\n\n\n\nspiece.model:   0%|          | 0.00/792k [00:00&lt;?, ?B/s]\n\n\n\ntokenizer.json:   0%|          | 0.00/2.42M [00:00&lt;?, ?B/s]\n\n\n\nspecial_tokens_map.json:   0%|          | 0.00/2.20k [00:00&lt;?, ?B/s]\n\n\nDevice set to use cuda:0\n</code></pre> <pre><code>prompt = \"Translate from English to French : Hi I am your best buddy since childhood\"\n\ntext2text_gen(prompt)\n</code></pre> <pre><code>[{'generated_text': 'Hi, je suis votre meilleur ami depuis la naissance'}]\n</code></pre> <pre><code># let's load llm tiiuae/falcon-7b-instruct and write prompt\nmodel = 'tiiuae/falcon-7b-instruct'\ntokenizer = AutoTokenizer.from_pretrained(model)\n</code></pre> <pre><code>device = 'cuda'\n</code></pre> <pre><code>!nvidia-smi\n</code></pre> <pre><code>Mon Feb 10 10:58:03 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   74C    P0             31W /   70W |    1690MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n</code></pre> <pre><code>pipe = pipeline(\"text-generation\",\n                model = model,\n                tokenizer = tokenizer,\n                torch_dtype = torch.bfloat16,\n                device=device)\n</code></pre> <pre><code>Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\nDevice set to use cuda\n</code></pre> <p>We have loaded our llm model until now via text-generation pipeline ok!.</p> <p>Let's learn to use the prompt for solving many NLP task with our llm model.</p> <p>1. Text Classification: In text classification task assign a label like positive, negative or neutral to a sequence of text which is also called sentiment analysis.</p> <p>Let's write a prompt that instructs the model to classify a given text</p> <pre><code>prompt = \"\"\"Classify the text into neutral, negative or positive.\nText: This movie is definitely one of my favorite movies of its kind.\nThe interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\nSentiment:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=10,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n</code></pre> <pre><code>Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\n\nResult: Classify the text into neutral, negative or positive. \nText: This movie is definitely one of my favorite movies of its kind. \nThe interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\nSentiment:\nPositive\n</code></pre> <p>2. Named Entity Recognition Named Entity Recognition (NER) is a task of finding named entities in a piece of text, such as a person, location, or organization.</p> <pre><code>prompt = \"\"\"Return a list of named entities in the text.\nText: The Golden State Warriors are an American professional basketball team based in San Francisco.\nNamed entities:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=15,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n</code></pre> <pre><code>Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\n\n- Golden State Warriors\n- San Francisco\n</code></pre> <p>3. Translation LLM (encoder-decoder transformer) can also perform the translation from one to another language. We will use Falcon-7b-instruct model and pass a prompt to instruct a model to translate a piece of text from English to Italian.  we\u2019ve added a do_sample=True and top_k=10 to allow the model to be a bit more flexible when generating output.</p> <pre><code>prompt = \"\"\"Translate the English text to Italian.\nText: Sometimes, I've believed as many as six impossible things before breakfast.\nTranslation:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=20,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n</code></pre> <pre><code>Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\n\nTalvolta, ho creduto fino a sei impossibili cose prima di colazione.\n</code></pre> <p>4. Text Summarization</p> <p>Text summarization is also an text generative task where output is heavily dependent on the input therefore we will use encoder-decoder transformer model but we can also use the decoder-only transformer for text summarization task also.</p> <p>Note : We can place the instruction either begining of prompt or end does't matter.</p> <pre><code>prompt = \"\"\"Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems. The principles and practices are drawn from traditional ecological knowledge of indigenous cultures combined with modern scientific understanding and technological innovations. Permaculture design provides a framework helping individuals and communities develop innovative, creative and effective strategies for meeting basic needs while preparing for and mitigating the projected impacts of climate change.\nWrite a summary of the above text.\nSummary:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=30,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n</code></pre> <pre><code>Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\n\nPermaculture is an ecological design approach mimicking nature's diversity, functionality, and resilience, to help individuals and communities cope with environmental challenges and prepare\n</code></pre> <p>5. Question Answering Prompt for question answering task can be structured as follows having logical componenets: instructions, context, questions, leading word/phrase like Answer: to tell the model start generationg answer.</p> <pre><code>prompt = \"\"\"Answer the question using the context below.\nContext: Gazpacho is a cold soup and drink made of raw, blended vegetables. Most gazpacho includes stale bread, tomato, cucumbers, onion, bell peppers, garlic, olive oil, wine vinegar, water, and salt. Northern recipes often include cumin and/or piment\u00f3n (smoked sweet paprika). Traditionally, gazpacho was made by pounding the vegetables in a mortar with a pestle; this more laborious method is still sometimes used as it helps keep the gazpacho cool and avoids the foam and silky consistency of smoothie versions made in blenders or food processors.\nQuestion: What modern tool is used to make gazpacho?\nAnswer:\n\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=10,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n</code></pre> <pre><code>Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\n\nResult: \nA blender or food processor is a modern tool\n</code></pre> <p>6. Reasoning As human have have build in ability to reason but we have to check does llm able to reason and we found that it's defficult for them to perform the reasoning but if we want to get better reasoning then we have to write the better prompts using prompting techniques like Chain-of-thought.</p> <pre><code># a model reason about a simple arithmetics task\nprompt = \"\"\"There are 5 groups of students in the class. Each group has 4 students. How many students are there in the class?\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=30,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n</code></pre> <pre><code>Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\n\nResult: \nThere are 5*4=20 groups in the class. Each group has 4 students. Therefore, there are 20*4\n</code></pre> <p>Let\u2019s increase the complexity a little and see if we can still get away... It's giving answer 8 which wrong becouse right answer is 12.</p> <pre><code>prompt = \"\"\"I baked 15 muffins. I ate 2 muffins and gave 5 muffins to a neighbor. My partner then bought 6 more muffins and ate 2. How many muffins do we now have?\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=10,\n    do_sample=True,\n    top_k=10,\n    return_full_text = False,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n</code></pre> <pre><code>Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\n\nResult: \nWe are left with 8 muffins.\n</code></pre> <p>Prompt Engineering Best Practices</p> <ol> <li>When choosing the model to work with, the latest and most capable models are likely to perform better.</li> <li>Start with a simple and short prompt, and iterate from there.</li> <li>Put the instructions at the beginning of the prompt, or at the very end. When working with large context, models apply various optimizations to prevent Attention complexity from scaling quadratically. This may make a model more attentive to the beginning or end of a prompt than the middle.</li> <li>Clearly separate instructions from the text they apply to - more on this in the next section.</li> <li>Be specific and descriptive about the task and the desired outcome - its format, length, style, language, etc.</li> <li>Avoid ambiguous descriptions and instructions.</li> <li>Favor instructions that say \u201cwhat to do\u201d instead of those that say \u201cwhat not to do\u201d.</li> <li>\u201cLead\u201d the output in the right direction by writing the first word (or even begin the first sentence for the model).</li> <li>Use advanced techniques like Few-shot prompting and Chain-of-thought Test your prompts with different models to assess their robustness. 10. Version and track the performance of your prompts.</li> </ol> <p>Zero-shot prompting model has been given instructions and context but no examples with solutions. Therefore, LLMs that have been fine-tuned on instruction datasets, generally perform well on such \u201czero-shot\u201d tasks.</p> <p>However, you may find that your task has more complexity or nuance, and, perhaps, you have some requirements for the output that the model doesn\u2019t catch on just from the instructions.  Then Few-shot prompting technique came into existance to tackle this problem ok</p> <p>Few-shot prompting we provide examples in the prompt giving the model more context to improve the performance. The examples condition the model to generate the output following the patterns in the examples.</p> <p>Limitations of the few-shot prompting technique:</p> <p>While LLMs can pick up on the patterns in the examples, these technique doesn\u2019t work well on complex reasoning tasks Few-shot prompting requires creating lengthy prompts. Prompts with large number of tokens can increase computation and latency. There\u2019s also a limit to the length of the prompts. Sometimes when given a number of examples, models can learn patterns that you didn\u2019t intend them to learn, e.g. that the third movie review is always negative.</p> <pre><code>prompt = \"\"\"Text: The first human went into space and orbited the Earth on April 12, 1961.\nDate: 04/12/1961\nText: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\nDate:\"\"\"\n\nsequences = pipe(\n    prompt,\n    max_new_tokens=8,\n    do_sample=True,\n    top_k=10,\n)\n\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n</code></pre> <pre><code>Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\n\nResult: Text: The first human went into space and orbited the Earth on April 12, 1961.\nDate: 04/12/1961\nText: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon. \nDate: 09/28/1960\n</code></pre> <p>Chain-of-thought Chain-of-thought (CoT) prompting is a technique that nudges a model to produce intermediate reasoning steps thus improving the results on complex reasoning tasks.</p> <p>There are two ways of steering a model to producing the reasoning steps:</p> <p>few-shot prompting by illustrating examples with detailed answers to questions, showing the model how to work through a problem. by instructing the model to reason by adding phrases like \u201cLet\u2019s think step by step\u201d or \u201cTake a deep breath and work through the problem step by step.\u201d</p> <pre><code>\n</code></pre> <p>Prompting vs Fine-tuning You can achieve great results by optimizing your prompts, however, you may still ponder whether fine-tuning a model would work better for your case. Here are some scenarios when fine-tuning a smaller model may be a preferred option:</p> <p>Your domain is wildly different from what LLMs were pre-trained on and extensive prompt optimization did not yield sufficient results. You need your model to work well in a low-resource language. You need the model to be trained on sensitive data that is under strict regulations. You have to use a small model due to cost, privacy, infrastructure or other limitations. In all of the above examples, you will need to make sure that you either already have or can easily obtain a large enough domain-specific dataset at a reasonable cost to fine-tune a model. You will also need to have enough time and resources to fine-tune a model.</p>"},{"location":"notebook/lab-prompting/#reference","title":"Reference","text":"<ol> <li>LLM Prompting Guide by HuggingFace</li> </ol>"}]}